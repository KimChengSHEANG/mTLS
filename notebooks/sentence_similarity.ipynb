{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/kim/.pyenv/versions/3.7.5/envs/ConLS/lib/python3.7/site-packages (from sentence-transformers) (4.6.1)\n",
      "Requirement already satisfied: tqdm in /home/kim/.pyenv/versions/3.7.5/envs/ConLS/lib/python3.7/site-packages (from sentence-transformers) (4.57.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/kim/.pyenv/versions/3.7.5/envs/ConLS/lib/python3.7/site-packages (from sentence-transformers) (1.8.1+cu111)\n",
      "Requirement already satisfied: torchvision in /home/kim/.pyenv/versions/3.7.5/envs/ConLS/lib/python3.7/site-packages (from sentence-transformers) (0.9.1+cu111)\n",
      "Requirement already satisfied: numpy in /home/kim/.pyenv/versions/3.7.5/envs/ConLS/lib/python3.7/site-packages (from sentence-transformers) (1.21.6)\n",
      "Requirement already satisfied: scikit-learn in /home/kim/.pyenv/versions/3.7.5/envs/ConLS/lib/python3.7/site-packages (from sentence-transformers) (1.0.2)\n",
      "Requirement already satisfied: scipy in /home/kim/.pyenv/versions/3.7.5/envs/ConLS/lib/python3.7/site-packages (from sentence-transformers) (1.7.3)\n",
      "Requirement already satisfied: nltk in /home/kim/.pyenv/versions/3.7.5/envs/ConLS/lib/python3.7/site-packages (from sentence-transformers) (3.4.5)\n",
      "Requirement already satisfied: sentencepiece in /home/kim/.pyenv/versions/3.7.5/envs/ConLS/lib/python3.7/site-packages (from sentence-transformers) (0.1.95)\n",
      "Collecting huggingface-hub>=0.4.0\n",
      "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /home/kim/.pyenv/versions/3.7.5/envs/ConLS/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.10.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/kim/.pyenv/versions/3.7.5/envs/ConLS/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/kim/.pyenv/versions/3.7.5/envs/ConLS/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\n",
      "Requirement already satisfied: filelock in /home/kim/.pyenv/versions/3.7.5/envs/ConLS/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\n",
      "Requirement already satisfied: requests in /home/kim/.pyenv/versions/3.7.5/envs/ConLS/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.25.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/kim/.pyenv/versions/3.7.5/envs/ConLS/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/kim/.pyenv/versions/3.7.5/envs/ConLS/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.10.3)\n",
      "Requirement already satisfied: sacremoses in /home/kim/.pyenv/versions/3.7.5/envs/ConLS/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.53)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /home/kim/.pyenv/versions/3.7.5/envs/ConLS/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
      "Requirement already satisfied: six in /home/kim/.pyenv/versions/3.7.5/envs/ConLS/lib/python3.7/site-packages (from nltk->sentence-transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/kim/.pyenv/versions/3.7.5/envs/ConLS/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/kim/.pyenv/versions/3.7.5/envs/ConLS/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/kim/.pyenv/versions/3.7.5/envs/ConLS/lib/python3.7/site-packages (from torchvision->sentence-transformers) (9.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/kim/.pyenv/versions/3.7.5/envs/ConLS/lib/python3.7/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/kim/.pyenv/versions/3.7.5/envs/ConLS/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.11.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/kim/.pyenv/versions/3.7.5/envs/ConLS/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kim/.pyenv/versions/3.7.5/envs/ConLS/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.9.24)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/kim/.pyenv/versions/3.7.5/envs/ConLS/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/kim/.pyenv/versions/3.7.5/envs/ConLS/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.13)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125919 sha256=b3822f943afb0ae8a3dc758c403e92ad4607aa113fc210c07c2ef189f5a7085c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-9lp73_av/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers, sentence-transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.10.3\n",
      "    Uninstalling tokenizers-0.10.3:\n",
      "      Successfully uninstalled tokenizers-0.10.3\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.0.8\n",
      "    Uninstalling huggingface-hub-0.0.8:\n",
      "      Successfully uninstalled huggingface-hub-0.0.8\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.6.1\n",
      "    Uninstalling transformers-4.6.1:\n",
      "      Successfully uninstalled transformers-4.6.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "easse 0.2.4 requires nltk>=3.6.5, but you have nltk 3.4.5 which is incompatible.\n",
      "easse 0.2.4 requires sacrebleu>=2.0.0, but you have sacrebleu 1.4.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface-hub-0.12.0 sentence-transformers-2.2.2 tokenizers-0.13.2 transformers-4.26.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/kim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/kim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# -- fix path --\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path('..').resolve()))\n",
    "\n",
    "from source.resources import *\n",
    "from source.metrics import *\n",
    "from source.helper import *\n",
    "from source.preprocessor import *\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from functools import lru_cache\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import magic\n",
    "from nltk.corpus import stopwords\n",
    "import json \n",
    "current_dir = Path('.')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> basic_transformer_models = ['albert-base-v1', 'albert-base-v2', 'albert-large-v1', 'albert-large-v2', 'albert-xlarge-v1', 'albert-xlarge-v2', 'albert-xxlarge-v1', 'albert-xxlarge-v2', 'bert-base-cased-finetuned-mrpc', 'bert-base-cased', 'bert-base-chinese', 'bert-base-german-cased', 'bert-base-german-dbmdz-cased', 'bert-base-german-dbmdz-uncased', 'bert-base-multilingual-cased', 'bert-base-multilingual-uncased', 'bert-base-uncased', 'bert-large-cased-whole-word-masking-finetuned-squad', 'bert-large-cased-whole-word-masking', 'bert-large-cased', 'bert-large-uncased-whole-word-masking-finetuned-squad', 'bert-large-uncased-whole-word-masking', 'bert-large-uncased', 'camembert-base', 'ctrl', 'distilbert-base-cased-distilled-squad', 'distilbert-base-cased', 'distilbert-base-german-cased', 'distilbert-base-multilingual-cased', 'distilbert-base-uncased-distilled-squad', 'distilbert-base-uncased-finetuned-sst-2-english', 'distilbert-base-uncased', 'distilgpt2', 'distilroberta-base', 'gpt2-large', 'gpt2-medium', 'gpt2-xl', 'gpt2', 'openai-gpt', 'roberta-base-openai-detector', 'roberta-base', 'roberta-large-mnli', 'roberta-large-openai-detector', 'roberta-large', 't5-11b', 't5-3b', 't5-base', 't5-large', 't5-small', 'transfo-xl-wt103', 'xlm-clm-ende-1024', 'xlm-clm-enfr-1024', 'xlm-mlm-100-1280', 'xlm-mlm-17-1280', 'xlm-mlm-en-2048', 'xlm-mlm-ende-1024', 'xlm-mlm-enfr-1024', 'xlm-mlm-enro-1024', 'xlm-mlm-tlm-xnli15-1024', 'xlm-mlm-xnli15-1024', 'xlm-roberta-base', 'xlm-roberta-large-finetuned-conll02-dutch', 'xlm-roberta-large-finetuned-conll02-spanish', 'xlm-roberta-large-finetuned-conll03-english', 'xlm-roberta-large-finetuned-conll03-german', 'xlm-roberta-large', 'xlnet-base-cased', 'xlnet-large-cased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cdbc070eec94adcb24ea5fd1db42b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)0fe39/.gitattributes:   0%|          | 0.00/968 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908abd327cb9471a960b360cc8c47215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348dd22ee10846d291c242833f118c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)83e900fe39/README.md:   0%|          | 0.00/3.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceff98afee404c9180633dd8bd62ca6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e900fe39/config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b72260cd60491c9fbb598e599e8c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ee7704dc2a467e94cbf355bb6895f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73ee2cd1f694dcd9b50718fef640e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45dc864675574c3888328f883302ac4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ncepiece.bpe.model\";:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21171f7502ee424a8bcf8d6c723d9d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98deb103646340d19824dddce459e9d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"tokenizer.json\";:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73cf2212b714b089e9b1e64ae350547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5730136ac64d05a78c648f41572a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"unigram.json\";:   0%|          | 0.00/14.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6552cc96497d4590ac5b299174e64789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)900fe39/modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2',  device=helper.get_device())\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',  device=helper.get_device())\n",
    "# model = SentenceTransformer('t5-large', device=device)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat sits outside                \t\t The dog plays in the garden \t\t Score: 0.2837\n",
      "A man is playing guitar             \t\t        A woman watches TV \t\t Score: -0.0327\n",
      "The new movie is awesome            \t\t The new movie is so great \t\t Score: 0.8940\n"
     ]
    }
   ],
   "source": [
    "# Two lists of sentences\n",
    "sentences1 = ['The cat sits outside',\n",
    "             'A man is playing guitar',\n",
    "             'The new movie is awesome']\n",
    "\n",
    "sentences2 = ['The dog plays in the garden',\n",
    "              'A woman watches TV',\n",
    "              'The new movie is so great']\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarities\n",
    "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "#Output the pairs with their score\n",
    "for i in range(len(sentences1)):\n",
    "    print(\"{:<35} \\t\\t {:>25} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>complex_word</th>\n",
       "      <th>complex_word_index</th>\n",
       "      <th>candidates</th>\n",
       "      <th>list_candidates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in march 1992 , linux version 0.95 was the fir...</td>\n",
       "      <td>pieces</td>\n",
       "      <td>34</td>\n",
       "      <td>[[parts], [bits], [components], [component, se...</td>\n",
       "      <td>[parts, bits, components, component, sections,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>much of the water carried by these streams is ...</td>\n",
       "      <td>diverted</td>\n",
       "      <td>9</td>\n",
       "      <td>[[redirected], [rerouted], [changed, moved], [...</td>\n",
       "      <td>[redirected, rerouted, changed, moved, drawn a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>harry also becomes the worthy possessor of the...</td>\n",
       "      <td>possessor</td>\n",
       "      <td>5</td>\n",
       "      <td>[[owner], [holder], [keeper], [buyer, master, ...</td>\n",
       "      <td>[owner, holder, keeper, buyer, master, teacher]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>escapologists escape from handcuffs , straitja...</td>\n",
       "      <td>perils</td>\n",
       "      <td>24</td>\n",
       "      <td>[[dangers], [difficulties, danger, restraints,...</td>\n",
       "      <td>[dangers, difficulties, danger, restraints, pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the storm continued , crossing the outer banks...</td>\n",
       "      <td>retained</td>\n",
       "      <td>13</td>\n",
       "      <td>[[kept], [held], [maintained], [regained]]</td>\n",
       "      <td>[kept, held, maintained, regained]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>the marriage was marked from the outset by sex...</td>\n",
       "      <td>outset</td>\n",
       "      <td>6</td>\n",
       "      <td>[[beginning], [start], [begining], [beggining,...</td>\n",
       "      <td>[beginning, start, begining, beggining, origin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>born as alfons karg , he was trained as a tele...</td>\n",
       "      <td>profession</td>\n",
       "      <td>19</td>\n",
       "      <td>[[job], [career], [work, job., specialty, occu...</td>\n",
       "      <td>[job, career, work, job., specialty, occupation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>some features , however , are reminiscent of t...</td>\n",
       "      <td>reminiscent</td>\n",
       "      <td>6</td>\n",
       "      <td>[[similar], [remindful], [reminders], [memorab...</td>\n",
       "      <td>[similar, remindful, reminders, memorable, sug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>the shinto pantheon alone consists of an uncou...</td>\n",
       "      <td>consists</td>\n",
       "      <td>4</td>\n",
       "      <td>[[is made], [is made up], [is], [contains], [i...</td>\n",
       "      <td>[is made, is made up, is, contains, is made up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>this message earned saddam a great deal of pop...</td>\n",
       "      <td>sectors</td>\n",
       "      <td>11</td>\n",
       "      <td>[[parts], [areas], [sections, regions], [places]]</td>\n",
       "      <td>[parts, areas, sections, regions, places]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text complex_word  \\\n",
       "0    in march 1992 , linux version 0.95 was the fir...       pieces   \n",
       "1    much of the water carried by these streams is ...     diverted   \n",
       "2    harry also becomes the worthy possessor of the...    possessor   \n",
       "3    escapologists escape from handcuffs , straitja...       perils   \n",
       "4    the storm continued , crossing the outer banks...     retained   \n",
       "..                                                 ...          ...   \n",
       "495  the marriage was marked from the outset by sex...       outset   \n",
       "496  born as alfons karg , he was trained as a tele...   profession   \n",
       "497  some features , however , are reminiscent of t...  reminiscent   \n",
       "498  the shinto pantheon alone consists of an uncou...     consists   \n",
       "499  this message earned saddam a great deal of pop...      sectors   \n",
       "\n",
       "     complex_word_index                                         candidates  \\\n",
       "0                    34  [[parts], [bits], [components], [component, se...   \n",
       "1                     9  [[redirected], [rerouted], [changed, moved], [...   \n",
       "2                     5  [[owner], [holder], [keeper], [buyer, master, ...   \n",
       "3                    24  [[dangers], [difficulties, danger, restraints,...   \n",
       "4                    13         [[kept], [held], [maintained], [regained]]   \n",
       "..                  ...                                                ...   \n",
       "495                   6  [[beginning], [start], [begining], [beggining,...   \n",
       "496                  19  [[job], [career], [work, job., specialty, occu...   \n",
       "497                   6  [[similar], [remindful], [reminders], [memorab...   \n",
       "498                   4  [[is made], [is made up], [is], [contains], [i...   \n",
       "499                  11  [[parts], [areas], [sections, regions], [places]]   \n",
       "\n",
       "                                       list_candidates  \n",
       "0    [parts, bits, components, component, sections,...  \n",
       "1    [redirected, rerouted, changed, moved, drawn a...  \n",
       "2      [owner, holder, keeper, buyer, master, teacher]  \n",
       "3    [dangers, difficulties, danger, restraints, pr...  \n",
       "4                   [kept, held, maintained, regained]  \n",
       "..                                                 ...  \n",
       "495    [beginning, start, begining, beggining, origin]  \n",
       "496   [job, career, work, job., specialty, occupation]  \n",
       "497  [similar, remindful, reminders, memorable, sug...  \n",
       "498  [is made, is made up, is, contains, is made up...  \n",
       "499          [parts, areas, sections, regions, places]  \n",
       "\n",
       "[500 rows x 5 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(Dataset.LexMTurk)\n",
    "data = update_candidates(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text in march 1992 , linux version 0.95 was the first to be capable of running x. this large version number jump was due to a feeling that a version 1.0 with no major missing pieces was imminent .\n",
      "complex_word pieces\n",
      "complex_word_index 34\n",
      "candidates [['parts'], ['bits'], ['components'], ['component', 'sections', 'elements', 'part', 'information', 'items']]\n",
      "list_candidates ['parts', 'bits', 'components', 'component', 'sections', 'elements', 'part', 'information', 'items']\n"
     ]
    }
   ],
   "source": [
    "row = data.iloc[0]\n",
    "# print(row)\n",
    "for key in row.keys():\n",
    "    # print(val)\n",
    "    print(key, row[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[40.2312]], device='cuda:0')\n",
      "99.94071125984192\n",
      "tensor([[39.6727]], device='cuda:0')\n",
      "96.97914123535156\n",
      "tensor([[40.3001]], device='cuda:0')\n",
      "98.94147515296936\n"
     ]
    }
   ],
   "source": [
    "# model = SentenceTransformer('all-MiniLM-L6-v2',  device=helper.get_device())\n",
    "# model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',  device=helper.get_device())\n",
    "model = SentenceTransformer('multi-qa-mpnet-base-dot-v1',  device=helper.get_device())\n",
    "def get_sentence_similarity(sent1, sent2):\n",
    "    sent1_embedding = model.encode(sent1, convert_to_tensor=True)\n",
    "    sent2_embedding = model.encode(sent2, convert_to_tensor=True)\n",
    "    return util.cos_sim(sent1_embedding, sent2_embedding).cpu().item() * 100\n",
    "sent1 = 'text in march 1992 , linux version 0.95 was the first to be capable of running x. this large version number jump was due to a feeling that a version 1.0 with no major missing pieces was imminent .'\n",
    "print(get_sentence_similarity(sent1, sent1.replace('pieces', 'parts')))\n",
    "print(get_sentence_similarity(sent1, sent1.replace('pieces', 'happy')))\n",
    "print(get_sentence_similarity(sent1, sent1.replace('pieces', 'boats')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.65068197250366\n",
      "97.53355383872986\n"
     ]
    }
   ],
   "source": [
    "sent1 = 'A comienzos de la década de 1980, se trasladó a Los Ángeles, en California, donde comenzó a labrarse una reputación con sus actuaciones, tanto eléctricas como acústicas.'\t\n",
    "\n",
    "sent2 = sent1.replace('labrarse', 'construirse') #\ttrabajar\n",
    "print(get_sentence_similarity(sent1, sent2))\n",
    "print(get_sentence_similarity(sent1, sent1.replace('acústicas', 'hola')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SentenceTransformer('all-MiniLM-L6-v2',  device=helper.get_device())\n",
    "# model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',  device=helper.get_device())\n",
    "model = SentenceTransformer('multi-qa-mpnet-base-dot-v1',  device=helper.get_device())\n",
    "def get_sentence_similarity(sent1, sent2):\n",
    "    sent1_embedding = model.encode(sent1, convert_to_tensor=True)\n",
    "    sent2_embedding = model.encode(sent2, convert_to_tensor=True)\n",
    "    return util.cos_sim(sent1_embedding, sent2_embedding).cpu().item() * 100, util.dot_score(sent1_embedding, sent2_embedding).cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "[['parts'], ['bits'], ['components'], ['component', 'sections', 'elements', 'part', 'information', 'items']]\n",
      "pieces parts                         \t:\t1.0000\t41.5298\n",
      "pieces bits                          \t:\t0.7087\t41.3486\n",
      "pieces components                    \t:\t0.9677\t41.5027\n",
      "pieces component                     \t:\t0.9668\t41.5508\n",
      "pieces sections                      \t:\t0.9179\t41.5969\n",
      "pieces elements                      \t:\t0.9881\t41.5639\n",
      "pieces part                          \t:\t0.9547\t41.5271\n",
      "pieces information                   \t:\t0.9788\t41.5120\n",
      "pieces items                         \t:\t0.9564\t41.4192\n",
      "pieces cat                           \t:\t0.0000\t40.9242\n",
      "================================================================================\n",
      "[['redirected'], ['rerouted'], ['changed', 'moved'], ['drawn away', 'turned', 'separated', 'switched', 'split', 'altered'], ['led away', 'sent away', 'veered', 'channeled', 'deflected']]\n",
      "diverted redirected                    \t:\t1.0000\t35.3048\n",
      "diverted rerouted                      \t:\t0.8943\t34.9638\n",
      "diverted changed                       \t:\t0.4182\t32.5767\n",
      "diverted moved                         \t:\t0.6255\t33.6189\n",
      "diverted drawn away                    \t:\t0.8180\t34.8083\n",
      "diverted turned                        \t:\t0.2493\t32.0358\n",
      "diverted separated                     \t:\t0.4617\t31.8982\n",
      "diverted switched                      \t:\t0.5777\t33.3041\n",
      "diverted split                         \t:\t0.4303\t32.1795\n",
      "diverted altered                       \t:\t0.3344\t32.2129\n",
      "diverted led away                      \t:\t0.4552\t33.6216\n",
      "diverted sent away                     \t:\t0.7301\t35.0997\n",
      "diverted veered                        \t:\t0.1222\t31.5965\n",
      "diverted channeled                     \t:\t0.8760\t34.0063\n",
      "diverted deflected                     \t:\t0.6442\t33.9264\n",
      "diverted cat                           \t:\t0.0000\t30.0397\n",
      "================================================================================\n",
      "[['owner'], ['holder'], ['keeper'], ['buyer', 'master', 'teacher']]\n",
      "possessor owner                         \t:\t0.8760\t40.9020\n",
      "possessor holder                        \t:\t1.0000\t40.7157\n",
      "possessor keeper                        \t:\t0.8764\t40.7449\n",
      "possessor buyer                         \t:\t0.6400\t40.3563\n",
      "possessor master                        \t:\t0.8138\t40.4069\n",
      "possessor teacher                       \t:\t0.1849\t39.7776\n",
      "possessor cat                           \t:\t0.0000\t39.4604\n",
      "================================================================================\n",
      "[['dangers'], ['difficulties', 'danger', 'restraints', 'problems', 'risks', 'hazards', 'fates', 'situations', 'troubles']]\n",
      "perils dangers                       \t:\t0.9320\t45.0754\n",
      "perils difficulties                  \t:\t0.5830\t44.2683\n",
      "perils danger                        \t:\t1.0000\t45.0699\n",
      "perils restraints                    \t:\t0.6557\t44.8236\n",
      "perils problems                      \t:\t0.6702\t44.4015\n",
      "perils risks                         \t:\t0.8126\t45.0599\n",
      "perils hazards                       \t:\t0.8931\t45.3231\n",
      "perils fates                         \t:\t0.7865\t44.0165\n",
      "perils situations                    \t:\t0.8784\t44.6352\n",
      "perils troubles                      \t:\t0.8084\t44.2863\n",
      "perils cat                           \t:\t0.0000\t43.5802\n",
      "================================================================================\n",
      "[['kept'], ['held'], ['maintained'], ['regained']]\n",
      "retained kept                          \t:\t0.9352\t44.0142\n",
      "retained held                          \t:\t0.9312\t44.0000\n",
      "retained maintained                    \t:\t1.0000\t44.0034\n",
      "retained regained                      \t:\t0.1341\t43.6438\n",
      "retained cat                           \t:\t0.0000\t43.6320\n",
      "================================================================================\n",
      "[['home'], ['house'], ['dwelling'], ['people', 'abode', 'owner']]\n",
      "residence home                          \t:\t0.9538\t45.5492\n",
      "residence house                         \t:\t0.8520\t45.0274\n",
      "residence dwelling                      \t:\t1.0000\t45.8624\n",
      "residence people                        \t:\t0.3708\t43.1764\n",
      "residence abode                         \t:\t0.9708\t45.8990\n",
      "residence owner                         \t:\t0.0000\t41.2526\n",
      "residence cat                           \t:\t0.3179\t42.7915\n",
      "================================================================================\n",
      "[['obtained'], ['gathered'], ['gotten'], ['grabbed'], ['made', 'acquired'], ['found', 'batten', 'aquired', 'created', 'attained', 'received', 'processed', 'collected', 'secured']]\n",
      "procured obtained                      \t:\t1.0000\t37.6846\n",
      "procured gathered                      \t:\t0.8454\t36.7279\n",
      "procured gotten                        \t:\t0.8712\t36.7587\n",
      "procured grabbed                       \t:\t0.8214\t36.4830\n",
      "procured made                          \t:\t0.6318\t35.5944\n",
      "procured acquired                      \t:\t0.9367\t37.5767\n",
      "procured found                         \t:\t0.7318\t36.5334\n",
      "procured batten                        \t:\t0.6777\t35.6606\n",
      "procured aquired                       \t:\t0.7934\t36.7594\n",
      "procured created                       \t:\t0.5567\t34.9171\n",
      "procured attained                      \t:\t0.9032\t37.3470\n",
      "procured received                      \t:\t0.7949\t36.9928\n",
      "procured processed                     \t:\t0.4839\t34.9236\n",
      "procured collected                     \t:\t0.7818\t36.2428\n",
      "procured secured                       \t:\t0.7634\t36.3196\n",
      "procured cat                           \t:\t0.0000\t31.2896\n",
      "================================================================================\n",
      "[['assembled', 'called'], ['gathered'], ['held'], ['started', 'organized', 'met'], ['opened', 'created', 'made'], ['aggregated', 'brought together', 'hosted', 'ordered']]\n",
      "convened assembled                     \t:\t1.0000\t46.8428\n",
      "convened called                        \t:\t0.5224\t46.5973\n",
      "convened gathered                      \t:\t0.9143\t46.8578\n",
      "convened held                          \t:\t0.8450\t46.8043\n",
      "convened started                       \t:\t0.2452\t46.3594\n",
      "convened organized                     \t:\t0.9683\t46.8939\n",
      "convened met                           \t:\t0.5265\t46.5908\n",
      "convened opened                        \t:\t0.0000\t46.1430\n",
      "convened created                       \t:\t0.0557\t46.1603\n",
      "convened made                          \t:\t0.6104\t46.5919\n",
      "convened aggregated                    \t:\t0.6804\t46.5723\n",
      "convened brought together              \t:\t0.6809\t46.6299\n",
      "convened hosted                        \t:\t0.1219\t46.0608\n",
      "convened ordered                       \t:\t0.4817\t46.6460\n",
      "convened cat                           \t:\t0.5066\t46.3463\n",
      "================================================================================\n",
      "[['important'], ['necessary'], ['essential', 'needed'], ['required'], ['critical'], ['crucial']]\n",
      "vital important                     \t:\t0.9013\t34.8675\n",
      "vital necessary                     \t:\t0.6280\t34.1643\n",
      "vital essential                     \t:\t0.9639\t35.6707\n",
      "vital needed                        \t:\t0.4978\t33.6713\n",
      "vital required                      \t:\t0.0000\t32.4083\n",
      "vital critical                      \t:\t0.9366\t35.0867\n",
      "vital crucial                       \t:\t1.0000\t35.2820\n",
      "vital cat                           \t:\t0.0047\t32.2390\n",
      "================================================================================\n",
      "[['gotten'], ['gained'], ['received', 'got'], ['obtained'], ['achieved'], ['taken'], ['amassed', 'started', 'inherited']]\n",
      "acquired gotten                        \t:\t0.7723\t44.5607\n",
      "acquired gained                        \t:\t0.8382\t44.7939\n",
      "acquired received                      \t:\t0.8165\t44.7001\n",
      "acquired got                           \t:\t0.7167\t44.6131\n",
      "acquired obtained                      \t:\t1.0000\t44.8732\n",
      "acquired achieved                      \t:\t0.4450\t44.6741\n",
      "acquired taken                         \t:\t0.8058\t44.7261\n",
      "acquired amassed                       \t:\t0.6182\t44.7089\n",
      "acquired started                       \t:\t0.0000\t44.3888\n",
      "acquired inherited                     \t:\t0.6220\t44.7493\n",
      "acquired cat                           \t:\t0.2948\t44.3578\n",
      "================================================================================\n",
      "[['amount'], ['plenty'], ['surplus'], ['large amount', 'supply', 'wealth'], ['bounty', 'excess', 'plenitude'], ['a lot', 'great amount', 'quantity', 'outpouring', 'portion', 'profusion', 'availability', 'heap', 'growth']]\n",
      "abundance amount                        \t:\t0.9922\t45.5206\n",
      "abundance plenty                        \t:\t0.9233\t45.3761\n",
      "abundance surplus                       \t:\t0.7746\t45.5442\n",
      "abundance large amount                  \t:\t0.9413\t45.4735\n",
      "abundance supply                        \t:\t1.0000\t45.5211\n",
      "abundance wealth                        \t:\t0.7157\t45.4002\n",
      "abundance bounty                        \t:\t0.9360\t45.3564\n",
      "abundance excess                        \t:\t0.8453\t45.5379\n",
      "abundance plenitude                     \t:\t0.9459\t45.4943\n",
      "abundance a lot                         \t:\t0.9127\t45.3722\n",
      "abundance great amount                  \t:\t0.9657\t45.4984\n",
      "abundance quantity                      \t:\t0.9913\t45.5150\n",
      "abundance outpouring                    \t:\t0.8520\t45.4815\n",
      "abundance portion                       \t:\t0.9269\t45.4395\n",
      "abundance profusion                     \t:\t0.9937\t45.4936\n",
      "abundance availability                  \t:\t0.9869\t45.4915\n",
      "abundance heap                          \t:\t0.9006\t45.4908\n",
      "abundance growth                        \t:\t0.6815\t45.3995\n",
      "abundance cat                           \t:\t0.0000\t45.1267\n",
      "================================================================================\n",
      "[['large'], ['significant'], ['big'], ['a lot'], ['huge', 'high', 'heavy', 'many', 'extensive', 'major', 'copious', 'great']]\n",
      "substantial large                         \t:\t0.9182\t43.2951\n",
      "substantial significant                   \t:\t1.0000\t43.3672\n",
      "substantial big                           \t:\t0.6432\t43.2113\n",
      "substantial a lot                         \t:\t0.0000\t43.0923\n",
      "substantial huge                          \t:\t0.6473\t43.1739\n",
      "substantial high                          \t:\t0.9073\t43.3656\n",
      "substantial heavy                         \t:\t0.8958\t43.3315\n",
      "substantial many                          \t:\t0.8691\t43.2783\n",
      "substantial extensive                     \t:\t0.9518\t43.3216\n",
      "substantial major                         \t:\t0.8697\t43.2912\n",
      "substantial copious                       \t:\t0.8575\t43.2395\n",
      "substantial great                         \t:\t0.8606\t43.2421\n",
      "substantial cat                           \t:\t0.4219\t43.2341\n",
      "================================================================================\n",
      "[['dry'], ['desolate', 'moistureless', 'barren']]\n",
      "arid dry                           \t:\t1.0000\t43.6175\n",
      "arid desolate                      \t:\t0.7469\t43.5826\n",
      "arid moistureless                  \t:\t0.6362\t43.5648\n",
      "arid barren                        \t:\t0.8722\t43.6079\n",
      "arid cat                           \t:\t0.0000\t42.8601\n",
      "================================================================================\n",
      "[['wandered'], ['drifted', 'walked'], ['traveled'], ['moved', 'strolled'], ['wondered', 'navigated', 'twisted', 'blew', 'went', 'curved', 'zigzagged']]\n",
      "meandered wandered                      \t:\t0.9235\t43.2690\n",
      "meandered drifted                       \t:\t1.0000\t42.9188\n",
      "meandered walked                        \t:\t0.6106\t42.7686\n",
      "meandered traveled                      \t:\t0.9091\t42.8787\n",
      "meandered moved                         \t:\t0.8654\t42.6447\n",
      "meandered strolled                      \t:\t0.8292\t43.3465\n",
      "meandered wondered                      \t:\t0.8755\t42.9361\n",
      "meandered navigated                     \t:\t0.9571\t43.2302\n",
      "meandered twisted                       \t:\t0.8642\t42.5081\n",
      "meandered blew                          \t:\t0.7983\t42.3039\n",
      "meandered went                          \t:\t0.8089\t42.4496\n",
      "meandered curved                        \t:\t0.8889\t42.9322\n",
      "meandered zigzagged                     \t:\t0.8674\t42.8556\n",
      "meandered cat                           \t:\t0.0000\t39.2589\n",
      "================================================================================\n",
      "[['created'], ['made'], ['produced'], ['yielded', 'formed', 'powered', 'had', 'achieved', 'built', 'developed']]\n",
      "generated created                       \t:\t0.9640\t45.8782\n",
      "generated made                          \t:\t0.8688\t45.8582\n",
      "generated produced                      \t:\t1.0000\t45.8545\n",
      "generated yielded                       \t:\t0.8559\t45.8625\n",
      "generated formed                        \t:\t0.7387\t45.8618\n",
      "generated powered                       \t:\t0.3598\t45.7944\n",
      "generated had                           \t:\t0.6801\t45.8122\n",
      "generated achieved                      \t:\t0.6627\t45.8409\n",
      "generated built                         \t:\t0.6100\t45.8317\n",
      "generated developed                     \t:\t0.7730\t45.8608\n",
      "generated cat                           \t:\t0.0000\t45.7359\n",
      "================================================================================\n",
      "[['make up'], ['form'], ['contain'], ['include'], ['includes', 'compose'], ['makes', 'consist of', 'contains', 'agreed', 'included', 'cover', 'create']]\n",
      "comprise make up                       \t:\t0.9910\t43.4434\n",
      "comprise form                          \t:\t0.9827\t43.2606\n",
      "comprise contain                       \t:\t0.7332\t42.9345\n",
      "comprise include                       \t:\t0.8865\t43.1776\n",
      "comprise includes                      \t:\t0.8619\t43.1656\n",
      "comprise compose                       \t:\t0.7281\t42.8145\n",
      "comprise makes                         \t:\t0.8365\t43.0641\n",
      "comprise consist of                    \t:\t1.0000\t43.3857\n",
      "comprise contains                      \t:\t0.7249\t42.9102\n",
      "comprise agreed                        \t:\t0.5161\t42.4299\n",
      "comprise included                      \t:\t0.8730\t43.1639\n",
      "comprise cover                         \t:\t0.0000\t42.3377\n",
      "comprise create                        \t:\t0.5337\t42.8455\n",
      "comprise cat                           \t:\t0.7068\t42.5019\n",
      "================================================================================\n",
      "[['holds'], ['fastens'], ['stops', 'protects', 'tightens', 'locks', 'fixes'], ['ties', 'binds', 'fixed', 'keeps', 'prevents', 'confines', 'limits', 'restrains', 'keep', 'traps', 'guards']]\n",
      "secures holds                         \t:\t0.8176\t44.9293\n",
      "secures fastens                       \t:\t0.9269\t45.0555\n",
      "secures stops                         \t:\t0.0000\t44.5436\n",
      "secures protects                      \t:\t0.8654\t45.0100\n",
      "secures tightens                      \t:\t0.6629\t44.8257\n",
      "secures locks                         \t:\t0.7341\t45.0738\n",
      "secures fixes                         \t:\t0.3850\t44.8312\n",
      "secures ties                          \t:\t0.7349\t44.9585\n",
      "secures binds                         \t:\t0.6187\t44.8909\n",
      "secures fixed                         \t:\t0.6066\t44.8263\n",
      "secures keeps                         \t:\t0.8130\t45.0072\n",
      "secures prevents                      \t:\t0.6023\t44.9088\n",
      "secures confines                      \t:\t0.8261\t44.9314\n",
      "secures limits                        \t:\t0.5365\t44.8076\n",
      "secures restrains                     \t:\t0.9688\t45.1332\n",
      "secures keep                          \t:\t0.8497\t44.9577\n",
      "secures traps                         \t:\t0.2260\t44.6294\n",
      "secures guards                        \t:\t1.0000\t45.1425\n",
      "secures cat                           \t:\t0.6673\t44.8911\n",
      "================================================================================\n",
      "[['job'], ['work'], ['life'], ['profession'], ['field'], ['line of work', 'honored', 'employment', 'occupation']]\n",
      "career job                           \t:\t0.9664\t44.5219\n",
      "career work                          \t:\t0.9870\t44.6744\n",
      "career life                          \t:\t0.9886\t44.6660\n",
      "career profession                    \t:\t1.0000\t44.7718\n",
      "career field                         \t:\t0.9756\t44.6222\n",
      "career line of work                  \t:\t0.9794\t44.6406\n",
      "career honored                       \t:\t0.7753\t43.7608\n",
      "career employment                    \t:\t0.9909\t44.5773\n",
      "career occupation                    \t:\t0.9455\t44.3700\n",
      "career cat                           \t:\t0.0000\t40.7815\n",
      "================================================================================\n",
      "[['declines'], ['denies'], ['does not want', 'avoids'], [\"doesn't\", 'decline'], ['decides not to', 'protests', 'ceases', 'disallowed', 'stopped', 'fails', 'declined', 'stops', 'says no', 'denied', \"doesn't want\", 'refrain', 'disagrees', 'decides not', \"won't\"]]\n",
      "refuses declines                      \t:\t1.0000\t45.0992\n",
      "refuses denies                        \t:\t0.6098\t44.9182\n",
      "refuses does not want                 \t:\t0.6688\t45.0311\n",
      "refuses avoids                        \t:\t0.6468\t45.0402\n",
      "refuses doesn't                       \t:\t0.7327\t45.0538\n",
      "refuses decline                       \t:\t0.8720\t45.1313\n",
      "refuses decides not to                \t:\t0.7968\t45.0630\n",
      "refuses protests                      \t:\t0.6102\t44.9312\n",
      "refuses ceases                        \t:\t0.6601\t44.9681\n",
      "refuses disallowed                    \t:\t0.0000\t45.0356\n",
      "refuses stopped                       \t:\t0.3451\t45.0338\n",
      "refuses fails                         \t:\t0.9753\t45.0409\n",
      "refuses declined                      \t:\t0.0430\t45.0674\n",
      "refuses stops                         \t:\t0.6813\t44.9361\n",
      "refuses says no                       \t:\t0.0123\t44.7381\n",
      "refuses denied                        \t:\t0.3592\t44.9766\n",
      "refuses doesn't want                  \t:\t0.5899\t45.1557\n",
      "refuses refrain                       \t:\t0.5202\t44.9503\n",
      "refuses disagrees                     \t:\t0.3434\t44.8784\n",
      "refuses decides not                   \t:\t0.9058\t45.0897\n",
      "refuses won't                         \t:\t0.5564\t45.0885\n",
      "refuses cat                           \t:\t0.1220\t44.9983\n",
      "================================================================================\n",
      "[['deaths'], ['injuries'], ['victims'], ['accidents', 'fatalities']]\n",
      "casualties deaths                        \t:\t0.9962\t40.4715\n",
      "casualties injuries                      \t:\t0.9484\t40.4297\n",
      "casualties victims                       \t:\t1.0000\t40.3702\n",
      "casualties accidents                     \t:\t0.8535\t40.2347\n",
      "casualties fatalities                    \t:\t0.9962\t40.5329\n",
      "casualties cat                           \t:\t0.0000\t37.3013\n",
      "================================================================================\n",
      "[['sending'], ['giving'], ['communicating', 'transferring'], ['conveying', 'spreading', 'relaying'], ['delivering', 'passing', 'moving', 'forwarding', 'sharing', 'broadcasting']]\n",
      "transmitting sending                       \t:\t0.8742\t40.9469\n",
      "transmitting giving                        \t:\t0.3532\t40.9104\n",
      "transmitting communicating                 \t:\t1.0000\t41.0918\n",
      "transmitting transferring                  \t:\t0.9226\t40.9949\n",
      "transmitting conveying                     \t:\t0.6548\t41.0266\n",
      "transmitting spreading                     \t:\t0.7831\t40.9812\n",
      "transmitting relaying                      \t:\t0.5705\t40.9754\n",
      "transmitting delivering                    \t:\t0.5797\t40.9887\n",
      "transmitting passing                       \t:\t0.5280\t40.9041\n",
      "transmitting moving                        \t:\t0.6249\t40.8059\n",
      "transmitting forwarding                    \t:\t0.5690\t40.8826\n",
      "transmitting sharing                       \t:\t0.4442\t40.9312\n",
      "transmitting broadcasting                  \t:\t0.8670\t41.0645\n",
      "transmitting cat                           \t:\t0.0000\t40.7928\n",
      "================================================================================\n",
      "[['sold'], ['available', 'put out'], ['delivered', 'distributed'], ['sent', 'published', 'issued'], ['initiated', 'created', 'made available', 'premiered', 'dispersed', 'given', 'launched', 'revealed', 'dispensed', 'available first', 'let out', 'provided', 'opened', 'let go', 'dropped', 'able to be sold', 'made public']]\n",
      "released sold                          \t:\t0.9450\t40.4036\n",
      "released available                     \t:\t0.9897\t40.9846\n",
      "released put out                       \t:\t1.0000\t40.9962\n",
      "released delivered                     \t:\t0.9566\t40.5316\n",
      "released distributed                   \t:\t0.9845\t40.7225\n",
      "released sent                          \t:\t0.9130\t40.2416\n",
      "released published                     \t:\t0.9873\t40.9635\n",
      "released issued                        \t:\t0.9781\t40.8796\n",
      "released initiated                     \t:\t0.7881\t40.4828\n",
      "released created                       \t:\t0.6707\t39.4383\n",
      "released made available                \t:\t0.9729\t41.0559\n",
      "released premiered                     \t:\t0.8545\t40.5271\n",
      "released dispersed                     \t:\t0.9504\t40.7101\n",
      "released given                         \t:\t0.9639\t40.7505\n",
      "released launched                      \t:\t0.8009\t40.1189\n",
      "released revealed                      \t:\t0.7635\t40.0355\n",
      "released dispensed                     \t:\t0.8585\t40.6453\n",
      "released available first               \t:\t0.9709\t40.8966\n",
      "released let out                       \t:\t0.9768\t40.8742\n",
      "released provided                      \t:\t0.9510\t40.7060\n",
      "released opened                        \t:\t0.8154\t40.3982\n",
      "released let go                        \t:\t0.5490\t38.8055\n",
      "released dropped                       \t:\t0.7589\t39.9145\n",
      "released able to be sold               \t:\t0.8700\t40.0640\n",
      "released made public                   \t:\t0.8600\t40.7372\n",
      "released cat                           \t:\t0.0000\t35.5072\n",
      "================================================================================\n",
      "[['situations'], ['cases'], ['instances'], ['ways', 'events'], ['examples', 'times', 'scenarios']]\n",
      "circumstances situations                    \t:\t1.0000\t36.2769\n",
      "circumstances cases                         \t:\t0.9867\t36.3858\n",
      "circumstances instances                     \t:\t0.9908\t36.3012\n",
      "circumstances ways                          \t:\t0.9016\t35.7347\n",
      "circumstances events                        \t:\t0.5282\t34.6985\n",
      "circumstances examples                      \t:\t0.8631\t35.4069\n",
      "circumstances times                         \t:\t0.9705\t36.2846\n",
      "circumstances scenarios                     \t:\t0.9860\t36.0354\n",
      "circumstances cat                           \t:\t0.0000\t33.3183\n",
      "================================================================================\n",
      "[['broke up'], ['separated'], ['dissolved'], ['ended', 'split'], ['scattered', 'quit', 'died', 'split up', 'stopped', 'broke apart']]\n",
      "disbanded broke up                      \t:\t0.8644\t44.9755\n",
      "disbanded separated                     \t:\t0.6640\t43.9129\n",
      "disbanded dissolved                     \t:\t1.0000\t45.5675\n",
      "disbanded ended                         \t:\t0.8043\t44.6718\n",
      "disbanded split                         \t:\t0.7214\t44.0018\n",
      "disbanded scattered                     \t:\t0.7505\t44.3446\n",
      "disbanded quit                          \t:\t0.7615\t44.5738\n",
      "disbanded died                          \t:\t0.6794\t43.9279\n",
      "disbanded split up                      \t:\t0.8443\t44.6650\n",
      "disbanded stopped                       \t:\t0.7762\t44.5840\n",
      "disbanded broke apart                   \t:\t0.7887\t44.4986\n",
      "disbanded cat                           \t:\t0.0000\t40.2535\n",
      "================================================================================\n",
      "[['imagined'], ['saw'], ['pictured'], ['envisioned'], ['anticipated'], ['considered', 'conceived', 'good', 'predicted', 'seen', 'featured']]\n",
      "envisaged imagined                      \t:\t0.9805\t43.4202\n",
      "envisaged saw                           \t:\t0.8670\t43.4646\n",
      "envisaged pictured                      \t:\t0.9215\t43.5785\n",
      "envisaged envisioned                    \t:\t1.0000\t43.4535\n",
      "envisaged anticipated                   \t:\t0.9766\t43.5402\n",
      "envisaged considered                    \t:\t0.8935\t43.3563\n",
      "envisaged conceived                     \t:\t0.9529\t43.3130\n",
      "envisaged good                          \t:\t0.5480\t42.5404\n",
      "envisaged predicted                     \t:\t0.7924\t43.1253\n",
      "envisaged seen                          \t:\t0.8858\t43.6099\n",
      "envisaged featured                      \t:\t0.7376\t42.8076\n",
      "envisaged cat                           \t:\t0.0000\t41.6273\n"
     ]
    }
   ],
   "source": [
    "def normalize(val, val_min, val_max):\n",
    "    return safe_division((val - val_min),(val_max - val_min))\n",
    "    \n",
    "tokenizer = get_tokenizer('en')    \n",
    "for i in range(len(data)):\n",
    "    if i == 25: break\n",
    "    print('='*80)\n",
    "    row = data.iloc[i]\n",
    "    source = row['text']\n",
    "    complex_word = row['complex_word']\n",
    "    tokens = tokenizer.tokenize(source)\n",
    "    index = tokens.index(complex_word)\n",
    "    window_size = 200\n",
    "    cropped_tokens = tokens[index-window_size:index+window_size+1]\n",
    "    source = ' '.join(cropped_tokens) \n",
    "    \n",
    "    # print(candidates)\n",
    "    candidates = row['list_candidates'] + ['cat']\n",
    "    print(row['candidates'])\n",
    "    source_embedding = model.encode(source, convert_to_tensor=True)\n",
    "    candidates_scores = []\n",
    "    for candidate in candidates: \n",
    "        target_sent = source.replace(complex_word, candidate)\n",
    "        cosine_score, dot_score = get_sentence_similarity(source, target_sent)\n",
    "        candidates_scores.append((candidate, cosine_score, dot_score))\n",
    "    # candidates_scores = sorted(candidates_scores, key=lambda x: x[1], reverse=True)\n",
    "    val_min = min(x[1] for x in candidates_scores)\n",
    "    val_max = max(x[1] for x in candidates_scores)\n",
    "    for candidate, cosine_score, dot_score in candidates_scores:\n",
    "        cosine_score = normalize(cosine_score, val_min, val_max)\n",
    "        print(f'{complex_word} {candidate:<30}\\t:\\t{cosine_score:.4f}\\t{dot_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from torch import cuda\n",
    "from functools import lru_cache\n",
    "\n",
    "def get_device_type():\n",
    "    device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "    # print(device)\n",
    "    device = torch.device(device)\n",
    "    \n",
    "@lru_cache(maxsize=1)\n",
    "def load_model():\n",
    "    BERT_MODEL = 'bert-large-uncased'\n",
    "    print(f'Load model: {BERT_MODEL}')\n",
    "    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\n",
    "    model = BertForMaskedLM.from_pretrained(BERT_MODEL)\n",
    "    device = get_device_type()\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_candidates(sent, complex_word, num_candidates=50):\n",
    "    model, tokenizer = load_model()\n",
    "    device = get_device_type()\n",
    "    sent = sent.lower()\n",
    "    masked_complex_sent = sent.replace(complex_word, \"[MASK]\") # replace complex word with [MASK]\n",
    "    transformed_sent = f'{sent} [SEP] {masked_complex_sent}' \n",
    "    encoding = tokenizer.encode_plus(transformed_sent, add_special_tokens=True)\n",
    "    # print(encoding)\n",
    "    mask_token_id = tokenizer.convert_tokens_to_ids('[MASK]')\n",
    "    masked_word_index = encoding['input_ids'].index(mask_token_id)\n",
    "    # print(mask_token_id, masked_word_index)\n",
    "    input_ids = torch.as_tensor([encoding[\"input_ids\"]]).to(device)\n",
    "    token_type_ids = torch.as_tensor([encoding[\"token_type_ids\"]]).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "        predictions = outputs['logits'][0][masked_word_index]\n",
    "        # print(predictions)\n",
    "        predicted_ids = torch.argsort(predictions, descending=True)[:num_candidates]\n",
    "        \n",
    "        # print(predictions[predicted_ids]) # the score of each predicted token\n",
    "        predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_ids)\n",
    "        predicted_scores = predictions[predicted_ids].cpu().numpy()\n",
    "        \n",
    "        return predicted_tokens, predicted_scores\n",
    "        # return dict(zip(predicted_tokens, predicted_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['sat', 'was', 'stood', 'perched', 'landed', 'slept', 'sits', 'jumped', 'remained', 'stayed', 'lay', 'crouched', 'fell', 'hopped', 'rested', 'spat', 'leaned', 'waited', 'climbed', 'lived', 'settled', 'died', 'sitting', 'stepped', 'sprawled', 'moved', 'sagged', 'sang', 'appeared', 'collapsed', 'flopped', 'hovered', 'stopped', 'lied', 'huddled', 'danced', 'parked', 'slumped', 'leaped', 'sleeping', 'dropped', 'knelt', 'rode', 'is', 'knocked', 'hung', 'perch', 'walked', 'floated', 'flew'], array([13.632623 , 11.902511 , 11.600557 , 11.281731 , 11.041062 ,\n",
      "       10.659115 , 10.493784 , 10.257942 , 10.177241 , 10.105614 ,\n",
      "       10.102157 ,  9.896737 ,  9.454384 ,  9.127811 ,  9.068984 ,\n",
      "        9.048487 ,  8.820898 ,  8.809768 ,  8.790929 ,  8.770737 ,\n",
      "        8.687552 ,  8.683246 ,  8.6137705,  8.449052 ,  8.390523 ,\n",
      "        8.359772 ,  8.258504 ,  8.258033 ,  8.250996 ,  8.235211 ,\n",
      "        8.231626 ,  8.132798 ,  8.012472 ,  8.003994 ,  7.9687157,\n",
      "        7.929854 ,  7.892858 ,  7.8567615,  7.815193 ,  7.8091593,\n",
      "        7.805304 ,  7.7521224,  7.7509136,  7.7169766,  7.599267 ,\n",
      "        7.5876427,  7.567798 ,  7.53259  ,  7.5080023,  7.3367267],\n",
      "      dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sent = 'the cat perched on the mat.'\n",
    "print(generate_candidates(sent, 'perched'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "in march 1992 , linux version 0.95 was the first to be capable of running x. this large version number jump was due to a feeling that a version 1.0 with no major missing pieces was imminent .\n",
      "pieces [['parts'], ['bits'], ['components'], ['component', 'sections', 'elements', 'part', 'information', 'items']]\n",
      "parts                         \t:\t0.9988\n",
      "piece                         \t:\t0.9987\n",
      "elements                      \t:\t0.9979\n",
      "points                        \t:\t0.9979\n",
      "links                         \t:\t0.9977\n",
      "components                    \t:\t0.9977\n",
      "items                         \t:\t0.9976\n",
      "bits                          \t:\t0.9974\n",
      "material                      \t:\t0.9969\n",
      "goods                         \t:\t0.9968\n",
      "================================================================================\n",
      "much of the water carried by these streams is diverted .\n",
      "diverted [['redirected'], ['rerouted'], ['changed', 'moved'], ['drawn away', 'turned', 'separated', 'switched', 'split', 'altered'], ['led away', 'sent away', 'veered', 'channeled', 'deflected']]\n",
      "pumped                        \t:\t0.9595\n",
      "discharged                    \t:\t0.9571\n",
      "diversion                     \t:\t0.9556\n",
      "treated                       \t:\t0.9475\n",
      "reclaimed                     \t:\t0.9468\n",
      "used                          \t:\t0.9454\n",
      "exported                      \t:\t0.9425\n",
      "reused                        \t:\t0.9406\n",
      "wasted                        \t:\t0.9368\n",
      "regulated                     \t:\t0.9367\n",
      "================================================================================\n",
      "harry also becomes the worthy possessor of the remaining deathly hallows : the invisibility cloak and the resurrection stone , hence becoming the true master of death .\n",
      "possessor [['owner'], ['holder'], ['keeper'], ['buyer', 'master', 'teacher']]\n",
      "owner                         \t:\t0.9958\n",
      "possess                       \t:\t0.9956\n",
      "possessing                    \t:\t0.9944\n",
      "proprietor                    \t:\t0.9942\n",
      "holder                        \t:\t0.9936\n",
      "possession                    \t:\t0.9932\n",
      "user                          \t:\t0.9928\n",
      "receiver                      \t:\t0.9921\n",
      "bearer                        \t:\t0.9918\n",
      "collector                     \t:\t0.9918\n",
      "================================================================================\n",
      "escapologists escape from handcuffs , straitjackets , cages , coffins , steel boxes , barrels , bags , burning buildings , fish-tanks and other perils , often in combination .\n",
      "perils [['dangers'], ['difficulties', 'danger', 'restraints', 'problems', 'risks', 'hazards', 'fates', 'situations', 'troubles']]\n",
      "hazards                       \t:\t0.9973\n",
      "dangers                       \t:\t0.9969\n",
      "risks                         \t:\t0.9963\n",
      "danger                        \t:\t0.9960\n",
      "circumstances                 \t:\t0.9957\n",
      "disasters                     \t:\t0.9952\n",
      "threats                       \t:\t0.9950\n",
      "crises                        \t:\t0.9947\n",
      "complications                 \t:\t0.9946\n",
      "events                        \t:\t0.9946\n",
      "================================================================================\n",
      "the storm continued , crossing the outer banks of north carolina , and retained its strength until june 20 when it became extratropical near newfoundland .\n",
      "retained [['kept'], ['held'], ['maintained'], ['regained']]\n",
      "preserved                     \t:\t0.9965\n",
      "held                          \t:\t0.9959\n",
      "lost                          \t:\t0.9955\n",
      "kept                          \t:\t0.9954\n",
      "recovered                     \t:\t0.9951\n",
      "maintained                    \t:\t0.9951\n",
      "contained                     \t:\t0.9951\n",
      "carried                       \t:\t0.9949\n",
      "sustained                     \t:\t0.9944\n",
      "gained                        \t:\t0.9940\n",
      "================================================================================\n",
      "the convent has been the official residence of the governor of gibraltar since 1728 .\n",
      "residence [['home'], ['house'], ['dwelling'], ['people', 'abode', 'owner']]\n",
      "house                         \t:\t0.9913\n",
      "household                     \t:\t0.9898\n",
      "seat                          \t:\t0.9895\n",
      "residency                     \t:\t0.9878\n",
      "quarters                      \t:\t0.9875\n",
      "workplace                     \t:\t0.9873\n",
      "compound                      \t:\t0.9860\n",
      "building                      \t:\t0.9855\n",
      "retreat                       \t:\t0.9853\n",
      "repository                    \t:\t0.9846\n",
      "================================================================================\n",
      "food is procured with its suckers and then crushed using its tough \" beak \" of chitin .\n",
      "procured [['obtained'], ['gathered'], ['gotten'], ['grabbed'], ['made', 'acquired'], ['found', 'batten', 'aquired', 'created', 'attained', 'received', 'processed', 'collected', 'secured']]\n",
      "acquired                      \t:\t0.9771\n",
      "obtained                      \t:\t0.9746\n",
      "produced                      \t:\t0.9709\n",
      "washed                        \t:\t0.9706\n",
      "retrieved                     \t:\t0.9697\n",
      "gained                        \t:\t0.9696\n",
      "harvested                     \t:\t0.9692\n",
      "accessed                      \t:\t0.9689\n",
      "sought                        \t:\t0.9679\n",
      "attached                      \t:\t0.9670\n",
      "================================================================================\n",
      "the united states convened a 13-nation conference of the international opium commission in 1909 in shanghai , china in response to increasing criticism of the opium trade .\n",
      "convened [['assembled', 'called'], ['gathered'], ['held'], ['started', 'organized', 'met'], ['opened', 'created', 'made'], ['aggregated', 'brought together', 'hosted', 'ordered']]\n",
      "assembled                     \t:\t0.9946\n",
      "instituted                    \t:\t0.9937\n",
      "conducted                     \t:\t0.9933\n",
      "held                          \t:\t0.9932\n",
      "chaired                       \t:\t0.9931\n",
      "summoned                      \t:\t0.9931\n",
      "organized                     \t:\t0.9926\n",
      "opened                        \t:\t0.9924\n",
      "initiated                     \t:\t0.9923\n",
      "dispatched                    \t:\t0.9921\n",
      "================================================================================\n",
      "photosynthesis is vital for life on earth .\n",
      "vital [['important'], ['necessary'], ['essential', 'needed'], ['required'], ['critical'], ['crucial']]\n",
      "essential                     \t:\t0.9910\n",
      "crucial                       \t:\t0.9890\n",
      "critical                      \t:\t0.9855\n",
      "imperative                    \t:\t0.9845\n",
      "important                     \t:\t0.9802\n",
      "fundamental                   \t:\t0.9795\n",
      "necessary                     \t:\t0.9791\n",
      "mandatory                     \t:\t0.9765\n",
      "basic                         \t:\t0.9710\n",
      "needed                        \t:\t0.9708\n",
      "================================================================================\n",
      "dodd simply retained his athletic director position , which he had acquired in 1950 .\n",
      "acquired [['gotten'], ['gained'], ['received', 'got'], ['obtained'], ['achieved'], ['taken'], ['amassed', 'started', 'inherited']]\n",
      "obtained                      \t:\t0.9959\n",
      "gained                        \t:\t0.9954\n",
      "earned                        \t:\t0.9947\n",
      "taken                         \t:\t0.9946\n",
      "purchased                     \t:\t0.9943\n",
      "received                      \t:\t0.9940\n",
      "adopted                       \t:\t0.9940\n",
      "established                   \t:\t0.9931\n",
      "secured                       \t:\t0.9929\n",
      "developed                     \t:\t0.9928\n",
      "================================================================================\n",
      "radiometric dating is a technique used to date materials , usually based on a comparison between the observed abundance of a naturally occurring radioactive isotope and its decay products , using known decay rates .\n",
      "abundance [['amount'], ['plenty'], ['surplus'], ['large amount', 'supply', 'wealth'], ['bounty', 'excess', 'plenitude'], ['a lot', 'great amount', 'quantity', 'outpouring', 'portion', 'profusion', 'availability', 'heap', 'growth']]\n",
      "amounts                       \t:\t0.9979\n",
      "concentration                 \t:\t0.9979\n",
      "quantity                      \t:\t0.9977\n",
      "quantities                    \t:\t0.9976\n",
      "density                       \t:\t0.9974\n",
      "frequency                     \t:\t0.9972\n",
      "prevalence                    \t:\t0.9971\n",
      "diversity                     \t:\t0.9971\n",
      "composition                   \t:\t0.9970\n",
      "number                        \t:\t0.9970\n",
      "================================================================================\n",
      "bacterial contaminants are ubiquitous , and foods left unused too long will often acquire substantial amounts of bacterial colonies and become dangerous to eat , leading to food poisoning .\n",
      "substantial [['large'], ['significant'], ['big'], ['a lot'], ['huge', 'high', 'heavy', 'many', 'extensive', 'major', 'copious', 'great']]\n",
      "considerable                  \t:\t0.9992\n",
      "significant                   \t:\t0.9992\n",
      "extensive                     \t:\t0.9983\n",
      "major                         \t:\t0.9980\n",
      "substantially                 \t:\t0.9979\n",
      "large                         \t:\t0.9978\n",
      "serious                       \t:\t0.9977\n",
      "widespread                    \t:\t0.9973\n",
      "severe                        \t:\t0.9968\n",
      "vast                          \t:\t0.9968\n",
      "================================================================================\n",
      "tibooburra has an arid , desert climate with temperatures soaring above 40 celsius in summer , often reaching as high as 47 c .\n",
      "arid [['dry'], ['desolate', 'moistureless', 'barren']]\n",
      "dry                           \t:\t0.9936\n",
      "humid                         \t:\t0.9915\n",
      "acidic                        \t:\t0.9912\n",
      "open                          \t:\t0.9910\n",
      "inland                        \t:\t0.9909\n",
      "tropical                      \t:\t0.9905\n",
      "extensive                     \t:\t0.9904\n",
      "intense                       \t:\t0.9901\n",
      "hot                           \t:\t0.9901\n",
      "agricultural                  \t:\t0.9900\n",
      "================================================================================\n",
      "hanna meandered around the southeastern bahamas , weakening to a tropical storm while also dumping heavy rain on already-devastated haiti .\n",
      "meandered [['wandered'], ['drifted', 'walked'], ['traveled'], ['moved', 'strolled'], ['wondered', 'navigated', 'twisted', 'blew', 'went', 'curved', 'zigzagged']]\n",
      "sped                          \t:\t0.9930\n",
      "twisted                       \t:\t0.9927\n",
      "rolled                        \t:\t0.9925\n",
      "wound                         \t:\t0.9922\n",
      "drifted                       \t:\t0.9920\n",
      "curled                        \t:\t0.9919\n",
      "swept                         \t:\t0.9919\n",
      "lingered                      \t:\t0.9919\n",
      "swung                         \t:\t0.9919\n",
      "slid                          \t:\t0.9916\n",
      "================================================================================\n",
      "with the high gulf pressures - a ship reported a pressure of 1015.5 millibars less than 60 m from the storm center at the time it was upgraded to a tropical storm - alicia was unable to gain size , staying very small , but generated faster winds , and became a category 1 hurricane on august 16\n",
      "generated [['created'], ['made'], ['produced'], ['yielded', 'formed', 'powered', 'had', 'achieved', 'built', 'developed']]\n",
      "recorded                      \t:\t0.9992\n",
      "produced                      \t:\t0.9989\n",
      "formed                        \t:\t0.9987\n",
      "harvested                     \t:\t0.9987\n",
      "developed                     \t:\t0.9986\n",
      "maintained                    \t:\t0.9986\n",
      "created                       \t:\t0.9985\n",
      "managed                       \t:\t0.9985\n",
      "provided                      \t:\t0.9985\n",
      "manufactured                  \t:\t0.9984\n",
      "================================================================================\n",
      "das rheingold is the first of the four operas that comprise der ring des nibelungen , by richard wagner .\n",
      "comprise [['make up'], ['form'], ['contain'], ['include'], ['includes', 'compose'], ['makes', 'consist of', 'contains', 'agreed', 'included', 'cover', 'create']]\n",
      "constitute                    \t:\t0.9961\n",
      "encompass                     \t:\t0.9959\n",
      "form                          \t:\t0.9951\n",
      "comprises                     \t:\t0.9948\n",
      "compose                       \t:\t0.9944\n",
      "composed                      \t:\t0.9943\n",
      "comprised                     \t:\t0.9942\n",
      "formed                        \t:\t0.9927\n",
      "represent                     \t:\t0.9927\n",
      "frame                         \t:\t0.9918\n",
      "================================================================================\n",
      "a frenulum is a small fold of tissue that secures or restricts the motion of a mobile organ in the body .\n",
      "secures [['holds'], ['fastens'], ['stops', 'protects', 'tightens', 'locks', 'fixes'], ['ties', 'binds', 'fixed', 'keeps', 'prevents', 'confines', 'limits', 'restrains', 'keep', 'traps', 'guards']]\n",
      "secure                        \t:\t0.9948\n",
      "maintains                     \t:\t0.9944\n",
      "anchors                       \t:\t0.9939\n",
      "retains                       \t:\t0.9934\n",
      "seals                         \t:\t0.9923\n",
      "preserves                     \t:\t0.9917\n",
      "aids                          \t:\t0.9914\n",
      "confines                      \t:\t0.9911\n",
      "holds                         \t:\t0.9909\n",
      "protects                      \t:\t0.9909\n",
      "================================================================================\n",
      "helen hunt has been recognized extensively in her career . in 1998 she become the second actress to win a golden globe award , an academy award and an emmy award in the same year .\n",
      "career [['job'], ['work'], ['life'], ['profession'], ['field'], ['line of work', 'honored', 'employment', 'occupation']]\n",
      "careers                       \t:\t0.9985\n",
      "work                          \t:\t0.9970\n",
      "life                          \t:\t0.9966\n",
      "years                         \t:\t0.9959\n",
      "achievements                  \t:\t0.9956\n",
      "business                      \t:\t0.9955\n",
      "success                       \t:\t0.9955\n",
      "industry                      \t:\t0.9954\n",
      "history                       \t:\t0.9954\n",
      "performances                  \t:\t0.9952\n",
      "================================================================================\n",
      "when wotan refuses to abandon his \" free hero \" , fricka lays bare his self-deception : siegmund is in no sense independent since his fate has been pre-ordained by wotan , who has even indirectly led him to find the magic sword .\n",
      "refuses [['declines'], ['denies'], ['does not want', 'avoids'], [\"doesn't\", 'decline'], ['decides not to', 'protests', 'ceases', 'disallowed', 'stopped', 'fails', 'declined', 'stops', 'says no', 'denied', \"doesn't want\", 'refrain', 'disagrees', 'decides not', \"won't\"]]\n",
      "fails                         \t:\t0.9988\n",
      "chooses                       \t:\t0.9983\n",
      "decides                       \t:\t0.9983\n",
      "declines                      \t:\t0.9981\n",
      "prefers                       \t:\t0.9979\n",
      "continues                     \t:\t0.9976\n",
      "wants                         \t:\t0.9975\n",
      "agrees                        \t:\t0.9975\n",
      "determines                    \t:\t0.9973\n",
      "proposes                      \t:\t0.9973\n",
      "================================================================================\n",
      "the storm never approached land during its lifespan , and no damage or casualties were reported .\n",
      "casualties [['deaths'], ['injuries'], ['victims'], ['accidents', 'fatalities']]\n",
      "fatalities                    \t:\t0.9936\n",
      "deaths                        \t:\t0.9905\n",
      "injuries                      \t:\t0.9892\n",
      "lives                         \t:\t0.9881\n",
      "losses                        \t:\t0.9875\n",
      "injury                        \t:\t0.9874\n",
      "mortality                     \t:\t0.9873\n",
      "death                         \t:\t0.9867\n",
      "survivors                     \t:\t0.9862\n",
      "victims                       \t:\t0.9859\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "    if i == 20: break\n",
    "    print('='*80)\n",
    "    row = data.iloc[i]\n",
    "    source = row['text']\n",
    "    complex_word = row['complex_word']\n",
    "    gold_candidates = row['candidates'] #row['list_candidates']\n",
    "    # print(candidates)\n",
    "    source_embedding = model.encode(source, convert_to_tensor=True)\n",
    "    candidates = generate_candidates(source, complex_word)[0]\n",
    "    candidates = remove_word_from_list(complex_word, candidates)\n",
    "    candidates_with_scores = []\n",
    "    for candidate in candidates: \n",
    "        # print(source, complex_word, candidate)\n",
    "        target_sent = source.replace(complex_word, candidate)\n",
    "        target_sent_embedding = model.encode(target_sent, convert_to_tensor=True)\n",
    "        cosine_score = util.cos_sim(source_embedding, target_sent_embedding).cpu().item()\n",
    "        candidates_with_scores.append((candidate, cosine_score))\n",
    "    candidates_with_scores = sorted(candidates_with_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(source)\n",
    "    print(complex_word, gold_candidates)\n",
    "    for candidate, score in candidates_with_scores[:10]:\n",
    "        print(f'{candidate:<30}\\t:\\t{score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat sits outside \t\t The dog plays in the garden \t\t Score: 0.2838\n",
      "A man is playing guitar \t\t A woman watches TV \t\t Score: -0.0327\n",
      "The new movie is awesome \t\t The new movie is so great \t\t Score: 0.8939\n",
      "The motive of the killing was not known. \t\t The reason of the killing was not known. \t\t Score: 0.9133\n",
      "The motive of the killing was not known. \t\t The tool of the killing was not known. \t\t Score: 0.8389\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Two lists of sentences\n",
    "sentences1 = ['The cat sits outside',\n",
    "             'A man is playing guitar',\n",
    "             'The new movie is awesome',\n",
    "             'The motive of the killing was not known.',\n",
    "             'The motive of the killing was not known.',]\n",
    "\n",
    "sentences2 = ['The dog plays in the garden',\n",
    "              'A woman watches TV',\n",
    "              'The new movie is so great',\n",
    "              'The reason of the killing was not known.',\n",
    "              'The tool of the killing was not known.',]\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarities\n",
    "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "#Output the pairs with their score\n",
    "for i in range(len(sentences1)):\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ConLS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "774dfef66576c1bc5f6c5092ce4ace6ecc473aab9c3e9d99e780abb421ef3888"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
