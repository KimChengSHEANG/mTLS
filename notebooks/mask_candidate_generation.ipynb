{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/kim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# -- fix path --\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path('..').resolve()))\n",
    "from source.resources import *\n",
    "from source.metrics import *\n",
    "from source.helper import *\n",
    "from source.preprocessor import *\n",
    "from source.constants import *\n",
    "import Levenshtein\n",
    "import wordfreq\n",
    "import torch\n",
    "from string import punctuation\n",
    "from nltk import word_tokenize\n",
    "from functools import lru_cache\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import magic\n",
    "import platform\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from joblib import Memory\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "current_dir = Path('.')\n",
    "memory = Memory(CACHE_DIR, verbose=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=5)\n",
    "def get_stopwords(language):\n",
    "    return set(stopwords.words(language))\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1024)\n",
    "def is_punctuation(word):\n",
    "    return not ''.join([char for char in word if char not in punctuation])\n",
    "\n",
    "def remove_punctuation(candidates):\n",
    "    return [candidate for candidate in candidates if not is_punctuation(candidate)]\n",
    "\n",
    "\n",
    "def remove_stopwords(candidates, language):\n",
    "    stopwords = get_stopwords(language)\n",
    "    return [candidate for candidate in candidates if candidate.lower() not in stopwords]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if platform.system() == 'Darwin' and torch.backends.mps.is_available():\n",
    "    device = 'mps' # Mac m-serie\n",
    "\n",
    "def replace(text, old, new):\n",
    "    pattern = re.compile(old, re.IGNORECASE)\n",
    "    return pattern.sub(new, text)\n",
    "\n",
    "def generate_candidates(model, text, top_k=10):\n",
    "    res = np.array(model(text, top_k=top_k))\n",
    "    if res.ndim > 1: res = res[0]\n",
    "    return [item['token_str'].strip() for item in res]\n",
    "\n",
    "@memory.cache()\n",
    "def eval_dataset(model_name, dataset, language, top_k=10, device=device):\n",
    "    nlp_fill = pipeline('fill-mask', model=model_name, tokenizer=model_name, device=device)\n",
    "    data = load_dataset(dataset)\n",
    "    data = update_candidates(data)\n",
    "    \n",
    "    list_pred_candidates = []\n",
    "    for i in tqdm(range(len(data)), total=len(data)):\n",
    "        row = data.iloc[i]\n",
    "        text = row['text']\n",
    "        complex_word = row['complex_word']\n",
    "        masked_text = replace(text, complex_word, nlp_fill.tokenizer.mask_token)\n",
    "        masked_text = f'{text} {nlp_fill.tokenizer.sep_token} {masked_text}'\n",
    "\n",
    "        candidates = generate_candidates(nlp_fill, masked_text, top_k=top_k+50)\n",
    "        candidates = remove_punctuation(candidates)\n",
    "        candidates = remove_stopwords(candidates, language)\n",
    "        list_pred_candidates.append(candidates[:top_k])\n",
    "        # print(text)\n",
    "        # print(complex_word, row['candidates'])\n",
    "        # print(candidates)\n",
    "        # print('='*80)\n",
    "        # break \n",
    "    data['mask_pred_candidates'] = list_pred_candidates\n",
    "    res = precision_metrics_at_k(data['mask_pred_candidates'], data['list_candidates'], k=top_k)\n",
    "    # print(res)\n",
    "    return data, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(models, dataset, language, top_k=50):\n",
    "    results_dir = RESOURCES_DIR / 'mask_pred_candidates'\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_name = f'{dataset}_topk_{top_k}'\n",
    "    \n",
    "    results = []\n",
    "    for model in models:\n",
    "        output_filepath = results_dir / f'{output_name}_{model.replace(\"/\", \"|\")}.csv'\n",
    "        if not output_filepath.exists():\n",
    "            try: \n",
    "                data, res = eval_dataset(model, dataset, language, top_k, device)\n",
    "            except NotImplementedError:\n",
    "                print(f'{model} is not implemented for mps, use cpu instead.')\n",
    "                data, res = eval_dataset(model, dataset, language, top_k, device='cpu')\n",
    "            \n",
    "            #preserve array\n",
    "            data['candidates'] = data['candidates'].apply(json.dumps)\n",
    "            data['list_candidates'] = data['list_candidates'].apply(json.dumps)\n",
    "            data['mask_pred_candidates'] = data['mask_pred_candidates'].apply(json.dumps)\n",
    "            \n",
    "            data.to_csv(output_filepath, index=False)\n",
    "            results.append((model,  res['potential'], res['recall'], res['precision']))\n",
    "        else:\n",
    "            data = pd.read_csv(output_filepath)\n",
    "            data['list_candidates'] = data['list_candidates'].apply(json.loads)\n",
    "            data['mask_pred_candidates'] = data['mask_pred_candidates'].apply(json.loads)\n",
    "            res = precision_metrics_at_k(data['mask_pred_candidates'], data['list_candidates'], k=top_k)\n",
    "            \n",
    "            acc1 = accuracy_at_1(data['mask_pred_candidates'], data['list_candidates'])\n",
    "            results.append((model, acc1,  res['potential'], res['recall'], res['precision']))\n",
    "            # print(f'{model:<50}: {value:.4f}')\n",
    "            \n",
    "        # print(f'{model:<50}: {res[\"potential\"]:.4f} {res[\"recall\"]:.4f} {res[\"precision\"]:.4f} ')\n",
    "\n",
    "    print(\"\\nSorted:\", '='*80)\n",
    "    print(f'Top k: {top_k}')\n",
    "    results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    scores_dir = current_dir / 'scores'\n",
    "    scores_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # print('\\t'.join(results.key()))\n",
    "    print('ACC1\\tPotential\\tRecall\\tPrecision')\n",
    "    scores = []\n",
    "    # with log_stdout(scores_dir / f'{output_name}.txt'):\n",
    "    for model, acc1, potential, recall, precision in results:\n",
    "        print(f'{model:<50}: {acc1:.4f} {potential:.4f} {recall:4f} {precision:.4f}')\n",
    "        # scores.append({'model':model, 'ACC@1': acc1, 'Potential':potential, 'Recall':recall, 'precision':precision})\n",
    "        scores.append({'model':model, 'Potential':potential, 'ACC@1': acc1})\n",
    "    pd.DataFrame(scores).to_csv(scores_dir / f'{output_name}.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 5\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "roberta-base                                      : 0.5518 0.9404 0.218587 0.4088\n",
      "bert-base-uncased                                 : 0.5518 0.8990 0.209554 0.3907\n",
      "bert-large-uncased                                : 0.5699 0.8938 0.207627 0.3845\n",
      "bert-large-cased                                  : 0.5596 0.8860 0.215043 0.3995\n",
      "roberta-large                                     : 0.5881 0.8575 0.176404 0.3363\n",
      "bert-base-cased                                   : 0.5311 0.8549 0.204418 0.3788\n",
      "distilbert-base-uncased                           : 0.5389 0.8549 0.192577 0.3591\n",
      "albert-base-v2                                    : 0.5207 0.8005 0.179119 0.3347\n",
      "xlm-roberta-large                                 : 0.4301 0.7073 0.135140 0.2518\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 10\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "roberta-base                                      : 0.5518 0.9715 0.311185 0.2972\n",
      "bert-large-uncased                                : 0.5699 0.9456 0.301236 0.2839\n",
      "bert-large-cased                                  : 0.5596 0.9456 0.301925 0.2878\n",
      "roberta-large                                     : 0.5881 0.9430 0.250888 0.2399\n",
      "bert-base-uncased                                 : 0.5518 0.9352 0.296741 0.2801\n",
      "distilbert-base-uncased                           : 0.5389 0.9171 0.272859 0.2585\n",
      "bert-base-cased                                   : 0.5311 0.9145 0.287786 0.2712\n",
      "albert-base-v2                                    : 0.5207 0.8679 0.247761 0.2337\n",
      "xlm-roberta-large                                 : 0.4301 0.7798 0.174428 0.1684\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 15\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "roberta-base                                      : 0.5518 0.9793 0.370047 0.2389\n",
      "bert-large-cased                                  : 0.5596 0.9637 0.353783 0.2273\n",
      "roberta-large                                     : 0.5881 0.9611 0.307231 0.1952\n",
      "bert-large-uncased                                : 0.5699 0.9534 0.357404 0.2276\n",
      "bert-base-uncased                                 : 0.5518 0.9456 0.349569 0.2228\n",
      "bert-base-cased                                   : 0.5311 0.9430 0.332872 0.2126\n",
      "distilbert-base-uncased                           : 0.5389 0.9249 0.322773 0.2052\n",
      "albert-base-v2                                    : 0.5207 0.8938 0.293307 0.1860\n",
      "xlm-roberta-large                                 : 0.4301 0.8109 0.201692 0.1309\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 20\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "roberta-base                                      : 0.5518 0.9819 0.407573 0.1997\n",
      "bert-large-cased                                  : 0.5596 0.9741 0.390276 0.1891\n",
      "roberta-large                                     : 0.5881 0.9741 0.346273 0.1668\n",
      "bert-base-uncased                                 : 0.5518 0.9637 0.387008 0.1864\n",
      "bert-large-uncased                                : 0.5699 0.9611 0.390150 0.1880\n",
      "bert-base-cased                                   : 0.5311 0.9508 0.365950 0.1768\n",
      "distilbert-base-uncased                           : 0.5389 0.9482 0.355803 0.1709\n",
      "albert-base-v2                                    : 0.5207 0.9171 0.326110 0.1561\n",
      "xlm-roberta-large                                 : 0.4301 0.8238 0.220780 0.1078\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 30\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "roberta-base                                      : 0.5518 0.9922 0.458915 0.1517\n",
      "bert-large-cased                                  : 0.5596 0.9845 0.433416 0.1411\n",
      "roberta-large                                     : 0.5881 0.9819 0.396490 0.1292\n",
      "bert-large-uncased                                : 0.5699 0.9741 0.442441 0.1445\n",
      "bert-base-uncased                                 : 0.5518 0.9689 0.437244 0.1416\n",
      "distilbert-base-uncased                           : 0.5389 0.9663 0.406491 0.1318\n",
      "bert-base-cased                                   : 0.5311 0.9637 0.414482 0.1343\n",
      "albert-base-v2                                    : 0.5207 0.9456 0.376367 0.1208\n",
      "xlm-roberta-large                                 : 0.4301 0.8394 0.237717 0.0778\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 40\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "roberta-base                                      : 0.5518 0.9948 0.495605 0.1240\n",
      "roberta-large                                     : 0.5881 0.9870 0.433299 0.1065\n",
      "bert-large-cased                                  : 0.5596 0.9845 0.468336 0.1152\n",
      "bert-base-uncased                                 : 0.5518 0.9793 0.472369 0.1153\n",
      "bert-large-uncased                                : 0.5699 0.9767 0.472175 0.1164\n",
      "distilbert-base-uncased                           : 0.5389 0.9741 0.440613 0.1078\n",
      "bert-base-cased                                   : 0.5311 0.9715 0.443772 0.1087\n",
      "albert-base-v2                                    : 0.5207 0.9560 0.410787 0.0994\n",
      "xlm-roberta-large                                 : 0.4301 0.8497 0.249788 0.0620\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 50\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "roberta-base                                      : 0.5518 0.9948 0.523896 0.1055\n",
      "roberta-large                                     : 0.5881 0.9896 0.458662 0.0909\n",
      "bert-large-cased                                  : 0.5596 0.9870 0.490599 0.0972\n",
      "bert-base-uncased                                 : 0.5518 0.9845 0.496417 0.0975\n",
      "bert-large-uncased                                : 0.5699 0.9819 0.496863 0.0983\n",
      "distilbert-base-uncased                           : 0.5389 0.9819 0.470242 0.0924\n",
      "bert-base-cased                                   : 0.5311 0.9741 0.468926 0.0923\n",
      "albert-base-v2                                    : 0.5207 0.9611 0.434523 0.0844\n",
      "xlm-roberta-large                                 : 0.4301 0.8523 0.258304 0.0517\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models = ['bert-base-uncased',\n",
    "          'bert-large-uncased',\n",
    "          'bert-base-cased',\n",
    "          'bert-large-cased',\n",
    "          'xlm-roberta-large',\n",
    "          'albert-base-v2',\n",
    "          'roberta-base',\n",
    "          'roberta-large',\n",
    "          'distilbert-base-uncased',\n",
    "        #   'bert-base-multilingual-cased',\n",
    "          ]\n",
    "for top_k in [5, 10, 15, 20, 30, 40, 50]:\n",
    "  run_experiments(models, Dataset.TSAR_EN, 'english', top_k=top_k)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 5\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "PlanTL-GOB-ES/roberta-large-bne                   : 0.4357 0.7743 0.169460 0.2955\n",
      "PlanTL-GOB-ES/roberta-base-bne                    : 0.3990 0.7717 0.160067 0.2761\n",
      "dccuchile/bert-base-spanish-wwm-cased             : 0.3963 0.7375 0.154160 0.2719\n",
      "dccuchile/albert-xxlarge-spanish                  : 0.3570 0.6745 0.135248 0.2346\n",
      "dccuchile/albert-base-spanish                     : 0.2782 0.6168 0.127575 0.2184\n",
      "xlm-roberta-large                                 : 0.3412 0.5879 0.105462 0.1874\n",
      "dccuchile/distilbert-base-spanish-uncased         : 0.3255 0.5827 0.114422 0.1953\n",
      "dccuchile/bert-base-spanish-wwm-uncased           : 0.3150 0.5696 0.111070 0.1895\n",
      "bert-base-multilingual-uncased                    : 0.2205 0.4646 0.080137 0.1407\n",
      "distilbert-base-multilingual-cased                : 0.1365 0.3176 0.049224 0.0861\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 10\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "PlanTL-GOB-ES/roberta-base-bne                    : 0.3990 0.8373 0.219590 0.1924\n",
      "PlanTL-GOB-ES/roberta-large-bne                   : 0.4357 0.8320 0.228657 0.2029\n",
      "dccuchile/bert-base-spanish-wwm-cased             : 0.3963 0.8163 0.203670 0.1832\n",
      "dccuchile/albert-xxlarge-spanish                  : 0.3570 0.7690 0.192861 0.1727\n",
      "dccuchile/albert-base-spanish                     : 0.2782 0.7375 0.177652 0.1564\n",
      "dccuchile/distilbert-base-spanish-uncased         : 0.3255 0.6640 0.150028 0.1307\n",
      "xlm-roberta-large                                 : 0.3412 0.6562 0.134973 0.1260\n",
      "dccuchile/bert-base-spanish-wwm-uncased           : 0.3150 0.6352 0.144958 0.1273\n",
      "bert-base-multilingual-uncased                    : 0.2205 0.5748 0.111475 0.1003\n",
      "distilbert-base-multilingual-cased                : 0.1365 0.4121 0.069822 0.0630\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 15\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "PlanTL-GOB-ES/roberta-base-bne                    : 0.3990 0.8688 0.255949 0.1536\n",
      "PlanTL-GOB-ES/roberta-large-bne                   : 0.4357 0.8661 0.262497 0.1587\n",
      "dccuchile/bert-base-spanish-wwm-cased             : 0.3963 0.8478 0.236478 0.1444\n",
      "dccuchile/albert-xxlarge-spanish                  : 0.3570 0.8189 0.225039 0.1363\n",
      "dccuchile/albert-base-spanish                     : 0.2782 0.7822 0.206106 0.1225\n",
      "dccuchile/distilbert-base-spanish-uncased         : 0.3255 0.6929 0.169455 0.0999\n",
      "dccuchile/bert-base-spanish-wwm-uncased           : 0.3150 0.6745 0.166621 0.0983\n",
      "xlm-roberta-large                                 : 0.3412 0.6693 0.148117 0.0931\n",
      "bert-base-multilingual-uncased                    : 0.2205 0.6115 0.124168 0.0749\n",
      "distilbert-base-multilingual-cased                : 0.1365 0.4724 0.086192 0.0518\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 20\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "PlanTL-GOB-ES/roberta-large-bne                   : 0.4357 0.8898 0.290563 0.1336\n",
      "PlanTL-GOB-ES/roberta-base-bne                    : 0.3990 0.8819 0.279409 0.1273\n",
      "dccuchile/bert-base-spanish-wwm-cased             : 0.3963 0.8688 0.259954 0.1201\n",
      "dccuchile/albert-xxlarge-spanish                  : 0.3570 0.8399 0.247193 0.1134\n",
      "dccuchile/albert-base-spanish                     : 0.2782 0.8005 0.224516 0.1009\n",
      "dccuchile/distilbert-base-spanish-uncased         : 0.3255 0.7218 0.183167 0.0822\n",
      "dccuchile/bert-base-spanish-wwm-uncased           : 0.3150 0.7008 0.180870 0.0808\n",
      "xlm-roberta-large                                 : 0.3412 0.6824 0.159802 0.0755\n",
      "bert-base-multilingual-uncased                    : 0.2205 0.6457 0.138093 0.0626\n",
      "distilbert-base-multilingual-cased                : 0.1365 0.5171 0.101006 0.0462\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 30\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "PlanTL-GOB-ES/roberta-large-bne                   : 0.4357 0.9055 0.322706 0.1003\n",
      "PlanTL-GOB-ES/roberta-base-bne                    : 0.3990 0.9003 0.315600 0.0973\n",
      "dccuchile/bert-base-spanish-wwm-cased             : 0.3963 0.8950 0.289757 0.0899\n",
      "dccuchile/albert-xxlarge-spanish                  : 0.3570 0.8583 0.275436 0.0856\n",
      "dccuchile/albert-base-spanish                     : 0.2782 0.8320 0.258112 0.0788\n",
      "dccuchile/distilbert-base-spanish-uncased         : 0.3255 0.7690 0.209598 0.0639\n",
      "dccuchile/bert-base-spanish-wwm-uncased           : 0.3150 0.7454 0.205121 0.0623\n",
      "xlm-roberta-large                                 : 0.3412 0.7034 0.175583 0.0559\n",
      "bert-base-multilingual-uncased                    : 0.2205 0.6772 0.157271 0.0483\n",
      "distilbert-base-multilingual-cased                : 0.1365 0.5617 0.119272 0.0370\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 40\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "PlanTL-GOB-ES/roberta-base-bne                    : 0.3990 0.9081 0.334110 0.0781\n",
      "PlanTL-GOB-ES/roberta-large-bne                   : 0.4357 0.9081 0.344266 0.0810\n",
      "dccuchile/bert-base-spanish-wwm-cased             : 0.3963 0.8976 0.305349 0.0716\n",
      "dccuchile/albert-xxlarge-spanish                  : 0.3570 0.8714 0.296010 0.0690\n",
      "dccuchile/albert-base-spanish                     : 0.2782 0.8504 0.274481 0.0635\n",
      "dccuchile/distilbert-base-spanish-uncased         : 0.3255 0.7900 0.223475 0.0515\n",
      "dccuchile/bert-base-spanish-wwm-uncased           : 0.3150 0.7690 0.219643 0.0503\n",
      "xlm-roberta-large                                 : 0.3412 0.7165 0.185069 0.0445\n",
      "bert-base-multilingual-uncased                    : 0.2205 0.7060 0.167678 0.0390\n",
      "distilbert-base-multilingual-cased                : 0.1365 0.6063 0.132226 0.0308\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 50\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "PlanTL-GOB-ES/roberta-large-bne                   : 0.4357 0.9239 0.361245 0.0685\n",
      "PlanTL-GOB-ES/roberta-base-bne                    : 0.3990 0.9213 0.347714 0.0655\n",
      "dccuchile/bert-base-spanish-wwm-cased             : 0.3963 0.9029 0.317328 0.0600\n",
      "dccuchile/albert-xxlarge-spanish                  : 0.3570 0.8871 0.310503 0.0584\n",
      "dccuchile/albert-base-spanish                     : 0.2782 0.8556 0.289894 0.0539\n",
      "dccuchile/distilbert-base-spanish-uncased         : 0.3255 0.8005 0.235585 0.0437\n",
      "dccuchile/bert-base-spanish-wwm-uncased           : 0.3150 0.7795 0.230963 0.0426\n",
      "xlm-roberta-large                                 : 0.3412 0.7244 0.192295 0.0371\n",
      "bert-base-multilingual-uncased                    : 0.2205 0.7165 0.176810 0.0329\n",
      "distilbert-base-multilingual-cased                : 0.1365 0.6273 0.144228 0.0270\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models = ['PlanTL-GOB-ES/roberta-base-bne',\n",
    "          'PlanTL-GOB-ES/roberta-large-bne',\n",
    "          'dccuchile/bert-base-spanish-wwm-cased', #BETO\n",
    "          'dccuchile/bert-base-spanish-wwm-uncased',\n",
    "          'dccuchile/distilbert-base-spanish-uncased',\n",
    "          'dccuchile/albert-base-spanish',\n",
    "          'dccuchile/albert-xxlarge-spanish',\n",
    "          'xlm-roberta-large',\n",
    "          'distilbert-base-multilingual-cased',\n",
    "          'bert-base-multilingual-uncased',\n",
    "        #   'flax-community/bertin-roberta-large-spanish',\n",
    "        #   'bertin-project/bertin-roberta-base-spanish',\n",
    "        #   'skimai/spanberta-base-cased',\n",
    "          ]\n",
    "for top_k in [5, 10, 15, 20, 30, 40, 50]:\n",
    "  run_experiments(models, Dataset.TSAR_ES, 'spanish', top_k=top_k)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portuguese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 5\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "neuralmind/bert-large-portuguese-cased            : 0.2850 0.7409 0.169373 0.2440\n",
      "neuralmind/bert-base-portuguese-cased             : 0.2642 0.7358 0.172531 0.2518\n",
      "xlm-roberta-large                                 : 0.3057 0.5725 0.119594 0.1762\n",
      "xlm-roberta-base                                  : 0.2513 0.5104 0.101851 0.1534\n",
      "rdenadai/BR_BERTo                                 : 0.1503 0.3834 0.065692 0.0974\n",
      "josu/roberta-pt-br                                : 0.0337 0.3368 0.062257 0.0860\n",
      "bert-base-multilingual-cased                      : 0.1528 0.3161 0.054858 0.0788\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 10\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "neuralmind/bert-large-portuguese-cased            : 0.2850 0.8394 0.240175 0.1767\n",
      "neuralmind/bert-base-portuguese-cased             : 0.2642 0.8109 0.234283 0.1772\n",
      "xlm-roberta-large                                 : 0.3057 0.6347 0.156321 0.1171\n",
      "xlm-roberta-base                                  : 0.2513 0.5959 0.134424 0.1031\n",
      "rdenadai/BR_BERTo                                 : 0.1503 0.4845 0.100751 0.0744\n",
      "josu/roberta-pt-br                                : 0.0337 0.4611 0.099472 0.0720\n",
      "bert-base-multilingual-cased                      : 0.1528 0.3860 0.071974 0.0536\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 15\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "neuralmind/bert-large-portuguese-cased            : 0.2850 0.8627 0.276274 0.1378\n",
      "neuralmind/bert-base-portuguese-cased             : 0.2642 0.8368 0.271803 0.1373\n",
      "xlm-roberta-large                                 : 0.3057 0.6632 0.176222 0.0900\n",
      "xlm-roberta-base                                  : 0.2513 0.6244 0.153680 0.0796\n",
      "rdenadai/BR_BERTo                                 : 0.1503 0.5622 0.128381 0.0629\n",
      "josu/roberta-pt-br                                : 0.0337 0.5104 0.122225 0.0598\n",
      "bert-base-multilingual-cased                      : 0.1528 0.4301 0.085726 0.0435\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 20\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "neuralmind/bert-large-portuguese-cased            : 0.2850 0.8679 0.301033 0.1140\n",
      "neuralmind/bert-base-portuguese-cased             : 0.2642 0.8472 0.297131 0.1137\n",
      "xlm-roberta-large                                 : 0.3057 0.6762 0.188004 0.0724\n",
      "xlm-roberta-base                                  : 0.2513 0.6477 0.167860 0.0657\n",
      "rdenadai/BR_BERTo                                 : 0.1503 0.6010 0.145235 0.0539\n",
      "josu/roberta-pt-br                                : 0.0337 0.5674 0.143532 0.0534\n",
      "bert-base-multilingual-cased                      : 0.1528 0.4560 0.094018 0.0360\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 30\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "neuralmind/bert-large-portuguese-cased            : 0.2850 0.8834 0.331603 0.0849\n",
      "neuralmind/bert-base-portuguese-cased             : 0.2642 0.8575 0.323456 0.0828\n",
      "xlm-roberta-large                                 : 0.3057 0.6891 0.210857 0.0547\n",
      "xlm-roberta-base                                  : 0.2513 0.6658 0.186559 0.0486\n",
      "rdenadai/BR_BERTo                                 : 0.1503 0.6632 0.178352 0.0451\n",
      "josu/roberta-pt-br                                : 0.0337 0.6140 0.167566 0.0421\n",
      "bert-base-multilingual-cased                      : 0.1528 0.4870 0.108525 0.0284\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 40\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "neuralmind/bert-large-portuguese-cased            : 0.2850 0.8912 0.344150 0.0665\n",
      "neuralmind/bert-base-portuguese-cased             : 0.2642 0.8782 0.342575 0.0664\n",
      "xlm-roberta-large                                 : 0.3057 0.7073 0.221056 0.0433\n",
      "rdenadai/BR_BERTo                                 : 0.1503 0.7047 0.206496 0.0392\n",
      "xlm-roberta-base                                  : 0.2513 0.6736 0.197650 0.0389\n",
      "josu/roberta-pt-br                                : 0.0337 0.6632 0.187838 0.0354\n",
      "bert-base-multilingual-cased                      : 0.1528 0.5130 0.117019 0.0230\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 50\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "neuralmind/bert-large-portuguese-cased            : 0.2850 0.8912 0.356191 0.0551\n",
      "neuralmind/bert-base-portuguese-cased             : 0.2642 0.8834 0.355761 0.0556\n",
      "rdenadai/BR_BERTo                                 : 0.1503 0.7332 0.226806 0.0346\n",
      "xlm-roberta-large                                 : 0.3057 0.7124 0.226088 0.0355\n",
      "xlm-roberta-base                                  : 0.2513 0.6943 0.207952 0.0330\n",
      "josu/roberta-pt-br                                : 0.0337 0.6813 0.207356 0.0314\n",
      "bert-base-multilingual-cased                      : 0.1528 0.5285 0.124681 0.0196\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "models = ['neuralmind/bert-base-portuguese-cased',\n",
    "          'neuralmind/bert-large-portuguese-cased',\n",
    "          'rdenadai/BR_BERTo',\n",
    "          'josu/roberta-pt-br',\n",
    "          'xlm-roberta-large',\n",
    "          'xlm-roberta-base',\n",
    "          'bert-base-multilingual-cased',\n",
    "        #   'bert-base-multilingual-uncased', \n",
    "        #   'facebook/xlm-roberta-xl',\n",
    "        #   'flax-community/alberti-bert-base-multilingual-cased'\n",
    "          \n",
    "          ]\n",
    "for top_k in [5, 10, 15, 20, 30, 40, 50]:\n",
    "  run_experiments(models, Dataset.TSAR_PT, 'portuguese', top_k=top_k)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 5\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "bert-large-cased                                  : 0.1340 0.9280 0.218528 0.3748\n",
      "bert-large-uncased                                : 0.1420 0.9260 0.211691 0.3684\n",
      "bert-base-uncased                                 : 0.1080 0.9220 0.210670 0.3652\n",
      "bert-base-cased                                   : 0.1400 0.9060 0.215749 0.3776\n",
      "xlm-roberta-large                                 : 0.4460 0.8960 0.208994 0.3748\n",
      "distilbert-base-uncased                           : 0.1360 0.8860 0.192959 0.3364\n",
      "roberta-base                                      : 0.0820 0.8740 0.181617 0.3156\n",
      "albert-base-v2                                    : 0.0700 0.8380 0.172271 0.2856\n",
      "roberta-large                                     : 0.0820 0.7220 0.129208 0.2260\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 10\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "bert-large-cased                                  : 0.1340 0.9740 0.319136 0.2998\n",
      "bert-base-uncased                                 : 0.1080 0.9720 0.319357 0.3006\n",
      "bert-large-uncased                                : 0.1420 0.9700 0.314169 0.2966\n",
      "roberta-base                                      : 0.0820 0.9700 0.285735 0.2684\n",
      "bert-base-cased                                   : 0.1400 0.9620 0.323027 0.2994\n",
      "distilbert-base-uncased                           : 0.1360 0.9500 0.278508 0.2620\n",
      "xlm-roberta-large                                 : 0.4460 0.9340 0.280659 0.2712\n",
      "albert-base-v2                                    : 0.0700 0.9260 0.253198 0.2300\n",
      "roberta-large                                     : 0.0820 0.9040 0.217445 0.2014\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 15\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "roberta-base                                      : 0.0820 0.9900 0.346264 0.2285\n",
      "bert-large-cased                                  : 0.1340 0.9820 0.381508 0.2489\n",
      "bert-base-uncased                                 : 0.1080 0.9800 0.380824 0.2501\n",
      "bert-large-uncased                                : 0.1420 0.9800 0.369671 0.2417\n",
      "bert-base-cased                                   : 0.1400 0.9700 0.377943 0.2439\n",
      "roberta-large                                     : 0.0820 0.9700 0.280018 0.1791\n",
      "distilbert-base-uncased                           : 0.1360 0.9620 0.338964 0.2216\n",
      "albert-base-v2                                    : 0.0700 0.9500 0.308353 0.1945\n",
      "xlm-roberta-large                                 : 0.4460 0.9440 0.316928 0.2105\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 20\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "roberta-base                                      : 0.0820 0.9920 0.386721 0.1986\n",
      "bert-large-uncased                                : 0.1420 0.9860 0.413366 0.2080\n",
      "bert-large-cased                                  : 0.1340 0.9860 0.426098 0.2140\n",
      "bert-base-uncased                                 : 0.1080 0.9820 0.415723 0.2094\n",
      "roberta-large                                     : 0.0820 0.9820 0.324637 0.1605\n",
      "bert-base-cased                                   : 0.1400 0.9760 0.418751 0.2093\n",
      "distilbert-base-uncased                           : 0.1360 0.9720 0.380463 0.1911\n",
      "albert-base-v2                                    : 0.0700 0.9640 0.346193 0.1680\n",
      "xlm-roberta-large                                 : 0.4460 0.9520 0.342092 0.1741\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 30\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "roberta-base                                      : 0.0820 0.9940 0.450177 0.1593\n",
      "bert-large-cased                                  : 0.1340 0.9920 0.487945 0.1679\n",
      "bert-large-uncased                                : 0.1420 0.9900 0.469115 0.1634\n",
      "bert-base-cased                                   : 0.1400 0.9880 0.469484 0.1609\n",
      "bert-base-uncased                                 : 0.1080 0.9860 0.474058 0.1633\n",
      "roberta-large                                     : 0.0820 0.9860 0.372926 0.1283\n",
      "distilbert-base-uncased                           : 0.1360 0.9780 0.430574 0.1483\n",
      "albert-base-v2                                    : 0.0700 0.9740 0.404096 0.1347\n",
      "xlm-roberta-large                                 : 0.4460 0.9640 0.371473 0.1294\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 40\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "roberta-base                                      : 0.0820 0.9980 0.494656 0.1338\n",
      "bert-base-uncased                                 : 0.1080 0.9960 0.508844 0.1341\n",
      "bert-large-uncased                                : 0.1420 0.9940 0.508440 0.1346\n",
      "bert-large-cased                                  : 0.1340 0.9940 0.522235 0.1375\n",
      "roberta-large                                     : 0.0820 0.9940 0.412014 0.1085\n",
      "bert-base-cased                                   : 0.1400 0.9900 0.504841 0.1328\n",
      "distilbert-base-uncased                           : 0.1360 0.9840 0.466834 0.1223\n",
      "albert-base-v2                                    : 0.0700 0.9780 0.439609 0.1121\n",
      "xlm-roberta-large                                 : 0.4460 0.9680 0.388282 0.1034\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 50\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "roberta-base                                      : 0.0820 1.0000 0.524214 0.1148\n",
      "roberta-large                                     : 0.0820 0.9980 0.444252 0.0953\n",
      "bert-base-uncased                                 : 0.1080 0.9960 0.535387 0.1147\n",
      "bert-large-uncased                                : 0.1420 0.9960 0.535903 0.1148\n",
      "bert-large-cased                                  : 0.1340 0.9960 0.546867 0.1170\n",
      "bert-base-cased                                   : 0.1400 0.9920 0.532631 0.1135\n",
      "distilbert-base-uncased                           : 0.1360 0.9860 0.497736 0.1050\n",
      "albert-base-v2                                    : 0.0700 0.9820 0.471313 0.0979\n",
      "xlm-roberta-large                                 : 0.4460 0.9680 0.400699 0.0865\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 5\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "bert-large-uncased                                : 0.0335 0.7364 0.232032 0.2192\n",
      "bert-base-uncased                                 : 0.0460 0.7280 0.238803 0.2234\n",
      "bert-large-cased                                  : 0.0418 0.7280 0.232387 0.2259\n",
      "bert-base-cased                                   : 0.0795 0.7197 0.235894 0.2259\n",
      "roberta-base                                      : 0.0502 0.7197 0.227931 0.2151\n",
      "distilbert-base-uncased                           : 0.0669 0.6569 0.198466 0.1967\n",
      "xlm-roberta-large                                 : 0.2092 0.6444 0.182412 0.1967\n",
      "albert-base-v2                                    : 0.0209 0.5858 0.175937 0.1674\n",
      "roberta-large                                     : 0.0502 0.5816 0.148707 0.1556\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 10\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "bert-base-uncased                                 : 0.0460 0.8870 0.355370 0.1837\n",
      "roberta-base                                      : 0.0502 0.8828 0.332818 0.1695\n",
      "bert-large-uncased                                : 0.0335 0.8787 0.351567 0.1824\n",
      "bert-base-cased                                   : 0.0795 0.8703 0.350262 0.1799\n",
      "bert-large-cased                                  : 0.0418 0.8577 0.334009 0.1782\n",
      "distilbert-base-uncased                           : 0.0669 0.7908 0.298476 0.1536\n",
      "albert-base-v2                                    : 0.0209 0.7615 0.289823 0.1414\n",
      "roberta-large                                     : 0.0502 0.7448 0.246928 0.1310\n",
      "xlm-roberta-large                                 : 0.2092 0.7113 0.243130 0.1439\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 15\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "bert-base-uncased                                 : 0.0460 0.9289 0.422747 0.1537\n",
      "bert-large-uncased                                : 0.0335 0.9205 0.411179 0.1490\n",
      "roberta-base                                      : 0.0502 0.9205 0.393016 0.1423\n",
      "bert-base-cased                                   : 0.0795 0.9121 0.397988 0.1442\n",
      "bert-large-cased                                  : 0.0418 0.8996 0.396522 0.1448\n",
      "distilbert-base-uncased                           : 0.0669 0.8536 0.358005 0.1308\n",
      "albert-base-v2                                    : 0.0209 0.8326 0.350539 0.1219\n",
      "roberta-large                                     : 0.0502 0.8285 0.303096 0.1138\n",
      "xlm-roberta-large                                 : 0.2092 0.7364 0.268259 0.1096\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 20\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "bert-base-uncased                                 : 0.0460 0.9456 0.460755 0.1289\n",
      "bert-large-cased                                  : 0.0418 0.9372 0.447957 0.1255\n",
      "roberta-base                                      : 0.0502 0.9372 0.436909 0.1236\n",
      "bert-large-uncased                                : 0.0335 0.9331 0.447948 0.1257\n",
      "bert-base-cased                                   : 0.0795 0.9247 0.432195 0.1213\n",
      "distilbert-base-uncased                           : 0.0669 0.8870 0.396044 0.1121\n",
      "roberta-large                                     : 0.0502 0.8745 0.345264 0.0990\n",
      "albert-base-v2                                    : 0.0209 0.8661 0.386092 0.1040\n",
      "xlm-roberta-large                                 : 0.2092 0.7573 0.286884 0.0921\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 30\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "bert-base-uncased                                 : 0.0460 0.9623 0.512508 0.0980\n",
      "bert-large-uncased                                : 0.0335 0.9540 0.508845 0.0990\n",
      "bert-large-cased                                  : 0.0418 0.9540 0.495417 0.0968\n",
      "roberta-base                                      : 0.0502 0.9540 0.499436 0.0985\n",
      "bert-base-cased                                   : 0.0795 0.9414 0.484737 0.0943\n",
      "distilbert-base-uncased                           : 0.0669 0.9289 0.462566 0.0884\n",
      "albert-base-v2                                    : 0.0209 0.9079 0.436640 0.0834\n",
      "roberta-large                                     : 0.0502 0.8954 0.392246 0.0780\n",
      "xlm-roberta-large                                 : 0.2092 0.7824 0.316508 0.0672\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 40\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "bert-large-cased                                  : 0.0418 0.9707 0.537061 0.0796\n",
      "bert-base-uncased                                 : 0.0460 0.9665 0.552257 0.0817\n",
      "bert-large-uncased                                : 0.0335 0.9665 0.543635 0.0815\n",
      "roberta-base                                      : 0.0502 0.9540 0.534197 0.0825\n",
      "bert-base-cased                                   : 0.0795 0.9456 0.516332 0.0769\n",
      "distilbert-base-uncased                           : 0.0669 0.9372 0.502776 0.0735\n",
      "albert-base-v2                                    : 0.0209 0.9289 0.486818 0.0709\n",
      "roberta-large                                     : 0.0502 0.9247 0.433161 0.0654\n",
      "xlm-roberta-large                                 : 0.2092 0.8075 0.334365 0.0541\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 50\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "bert-base-uncased                                 : 0.0460 0.9749 0.577074 0.0697\n",
      "bert-large-uncased                                : 0.0335 0.9707 0.561680 0.0686\n",
      "bert-large-cased                                  : 0.0418 0.9707 0.556937 0.0674\n",
      "roberta-base                                      : 0.0502 0.9623 0.553098 0.0699\n",
      "bert-base-cased                                   : 0.0795 0.9582 0.553478 0.0665\n",
      "distilbert-base-uncased                           : 0.0669 0.9582 0.543878 0.0633\n",
      "albert-base-v2                                    : 0.0209 0.9414 0.518558 0.0623\n",
      "roberta-large                                     : 0.0502 0.9414 0.465763 0.0569\n",
      "xlm-roberta-large                                 : 0.2092 0.8117 0.338518 0.0445\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 5\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "bert-large-uncased                                : 0.0678 0.8310 0.281142 0.3001\n",
      "bert-large-cased                                  : 0.0657 0.8245 0.282564 0.3001\n",
      "bert-base-cased                                   : 0.0829 0.8159 0.277738 0.3012\n",
      "bert-base-uncased                                 : 0.0517 0.8138 0.271075 0.2921\n",
      "roberta-base                                      : 0.0312 0.7847 0.256898 0.2637\n",
      "distilbert-base-uncased                           : 0.0721 0.7783 0.245113 0.2663\n",
      "xlm-roberta-large                                 : 0.3229 0.7449 0.242510 0.2784\n",
      "albert-base-v2                                    : 0.0280 0.7277 0.221986 0.2304\n",
      "roberta-large                                     : 0.0323 0.6319 0.171830 0.1813\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 10\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "bert-large-cased                                  : 0.0657 0.9182 0.404545 0.2333\n",
      "bert-large-uncased                                : 0.0678 0.9085 0.404140 0.2321\n",
      "roberta-base                                      : 0.0312 0.9064 0.384121 0.2155\n",
      "bert-base-uncased                                 : 0.0517 0.8988 0.399703 0.2316\n",
      "bert-base-cased                                   : 0.0829 0.8934 0.398720 0.2294\n",
      "distilbert-base-uncased                           : 0.0721 0.8687 0.353684 0.2037\n",
      "albert-base-v2                                    : 0.0280 0.8504 0.328287 0.1835\n",
      "roberta-large                                     : 0.0323 0.8299 0.286033 0.1598\n",
      "xlm-roberta-large                                 : 0.3229 0.8127 0.316400 0.1964\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 15\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "bert-large-cased                                  : 0.0657 0.9419 0.472266 0.1891\n",
      "roberta-base                                      : 0.0312 0.9333 0.450820 0.1789\n",
      "bert-large-uncased                                : 0.0678 0.9322 0.467146 0.1854\n",
      "bert-base-uncased                                 : 0.0517 0.9171 0.470072 0.1896\n",
      "bert-base-cased                                   : 0.0829 0.9096 0.460116 0.1841\n",
      "roberta-large                                     : 0.0323 0.8977 0.359401 0.1393\n",
      "distilbert-base-uncased                           : 0.0721 0.8902 0.419541 0.1685\n",
      "albert-base-v2                                    : 0.0280 0.8837 0.389076 0.1513\n",
      "xlm-roberta-large                                 : 0.3229 0.8267 0.349423 0.1501\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 20\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "bert-large-cased                                  : 0.0657 0.9494 0.523527 0.1612\n",
      "bert-large-uncased                                : 0.0678 0.9483 0.514869 0.1577\n",
      "roberta-base                                      : 0.0312 0.9429 0.492763 0.1525\n",
      "bert-base-uncased                                 : 0.0517 0.9279 0.507325 0.1570\n",
      "bert-base-cased                                   : 0.0829 0.9236 0.504133 0.1559\n",
      "roberta-large                                     : 0.0323 0.9182 0.407714 0.1228\n",
      "albert-base-v2                                    : 0.0280 0.9128 0.435043 0.1295\n",
      "distilbert-base-uncased                           : 0.0721 0.9074 0.464041 0.1434\n",
      "xlm-roberta-large                                 : 0.3229 0.8332 0.371559 0.1227\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 30\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "bert-large-uncased                                : 0.0678 0.9634 0.573145 0.1216\n",
      "bert-large-cased                                  : 0.0657 0.9580 0.575350 0.1230\n",
      "roberta-base                                      : 0.0312 0.9526 0.556218 0.1201\n",
      "bert-base-uncased                                 : 0.0517 0.9440 0.565345 0.1204\n",
      "bert-base-cased                                   : 0.0829 0.9429 0.557282 0.1185\n",
      "roberta-large                                     : 0.0323 0.9376 0.462449 0.0966\n",
      "distilbert-base-uncased                           : 0.0721 0.9311 0.523782 0.1107\n",
      "albert-base-v2                                    : 0.0280 0.9279 0.489557 0.1012\n",
      "xlm-roberta-large                                 : 0.3229 0.8601 0.403151 0.0908\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 40\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "bert-large-uncased                                : 0.0678 0.9688 0.612536 0.0992\n",
      "bert-large-cased                                  : 0.0657 0.9688 0.609898 0.0999\n",
      "roberta-base                                      : 0.0312 0.9569 0.598918 0.0996\n",
      "bert-base-uncased                                 : 0.0517 0.9537 0.602928 0.0983\n",
      "bert-base-cased                                   : 0.0829 0.9526 0.590989 0.0966\n",
      "roberta-large                                     : 0.0323 0.9526 0.507757 0.0811\n",
      "albert-base-v2                                    : 0.0280 0.9408 0.527648 0.0836\n",
      "distilbert-base-uncased                           : 0.0721 0.9408 0.561348 0.0905\n",
      "xlm-roberta-large                                 : 0.3229 0.8687 0.418819 0.0722\n",
      "\n",
      "Sorted: ================================================================================\n",
      "Top k: 50\n",
      "ACC1\tPotential\tRecall\tPrecision\n",
      "bert-large-cased                                  : 0.0657 0.9720 0.634946 0.0844\n",
      "bert-large-uncased                                : 0.0678 0.9699 0.639862 0.0841\n",
      "roberta-base                                      : 0.0312 0.9623 0.629163 0.0848\n",
      "bert-base-cased                                   : 0.0829 0.9591 0.618567 0.0820\n",
      "bert-base-uncased                                 : 0.0517 0.9580 0.627558 0.0833\n",
      "roberta-large                                     : 0.0323 0.9580 0.541338 0.0704\n",
      "albert-base-v2                                    : 0.0280 0.9505 0.563625 0.0726\n",
      "distilbert-base-uncased                           : 0.0721 0.9494 0.590811 0.0771\n",
      "xlm-roberta-large                                 : 0.3229 0.8698 0.428512 0.0599\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models = ['bert-base-uncased',\n",
    "          'bert-large-uncased',\n",
    "          'bert-base-cased',\n",
    "          'bert-large-cased',\n",
    "          'xlm-roberta-large',\n",
    "          'albert-base-v2',\n",
    "          'roberta-base',\n",
    "          'roberta-large',\n",
    "          'distilbert-base-uncased',\n",
    "        #   'bert-base-multilingual-cased',\n",
    "          ]\n",
    "for dataset in [Dataset.LexMTurk, Dataset.NNSeval, Dataset.BenchLS]:\n",
    "  for top_k in [5, 10, 15, 20, 30, 40, 50]:\n",
    "    run_experiments(models, dataset, 'english', top_k=top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ConLS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
